{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìã Notebook 02: Data Preparation IMPROVED\n",
    "\n",
    "**Objective:** Prepare two versions of the dataset for modeling\n",
    "\n",
    "**What we'll do:**\n",
    "1. Load the clean data from Notebook 01\n",
    "2. Create Dataset B (Full) - all 21 features\n",
    "3. Create Dataset A (Clean) - remove potentially leaky features\n",
    "4. Prepare scaling strategy\n",
    "5. Save both datasets\n",
    "\n",
    "**Why two datasets?**\n",
    "- Dataset B (Full): Shows maximum predictive power (but might include target leakage)\n",
    "- Dataset A (Clean): More realistic for preventive screening (removes consequences of diabetes)\n",
    "- Comparing them demonstrates critical thinking about feature selection!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For scaling (we'll prepare the strategy, not fit yet)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 2: Load Data from Notebook 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Shape: (253680, 22)\n",
      "\n",
      "Columns: ['Diabetes_012', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\yaros\\\\Desktop\\\\python\\\\faidm\\\\individual_project\\\\diabetes-classification-ml\\\\data\\\\CDC Diabetes Dataset.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA QUALITY VERIFICATION\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ BASIC CHECKS\n",
      "------------------------------------------------------------\n",
      "Total rows: 253,680\n",
      "Total columns: 22\n",
      "Missing values: 0\n",
      "Duplicate rows: 23899\n",
      "\n",
      "2Ô∏è‚É£ DATA TYPES\n",
      "------------------------------------------------------------\n",
      "float64    22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚ö†Ô∏è All columns should be numeric (float64 or int64)\n",
      "\n",
      "3Ô∏è‚É£ NON-NUMERIC VALUES CHECK\n",
      "------------------------------------------------------------\n",
      "‚úÖ All columns are numeric\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive data quality check\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Basic checks\n",
    "print(\"\\n1Ô∏è‚É£ BASIC CHECKS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# 2. Data types check\n",
    "print(\"\\n2Ô∏è‚É£ DATA TYPES\")\n",
    "print(\"-\" * 60)\n",
    "print(df.dtypes.value_counts())\n",
    "print(f\"\\n‚ö†Ô∏è All columns should be numeric (float64 or int64)\")\n",
    "\n",
    "# 3. Check for any non-numeric values\n",
    "print(\"\\n3Ô∏è‚É£ NON-NUMERIC VALUES CHECK\")\n",
    "print(\"-\" * 60)\n",
    "non_numeric_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
    "if non_numeric_cols:\n",
    "    print(f\"‚ö†Ô∏è Non-numeric columns found: {non_numeric_cols}\")\n",
    "else:\n",
    "    print(\"‚úÖ All columns are numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4Ô∏è‚É£ VALUE RANGE VERIFICATION\n",
      "------------------------------------------------------------\n",
      "Checking if values are within expected ranges...\n",
      "\n",
      "‚úÖ All features are within expected ranges\n"
     ]
    }
   ],
   "source": [
    "# 4. Check value ranges for each feature\n",
    "print(\"\\n4Ô∏è‚É£ VALUE RANGE VERIFICATION\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Checking if values are within expected ranges...\\n\")\n",
    "\n",
    "# Expected ranges based on dataset description\n",
    "expected_ranges = {\n",
    "    'Diabetes_012': (0, 2),\n",
    "    'HighBP': (0, 1),\n",
    "    'HighChol': (0, 1),\n",
    "    'CholCheck': (0, 1),\n",
    "    'BMI': (12, 98),  # Reasonable human BMI range\n",
    "    'Smoker': (0, 1),\n",
    "    'Stroke': (0, 1),\n",
    "    'HeartDiseaseorAttack': (0, 1),\n",
    "    'PhysActivity': (0, 1),\n",
    "    'Fruits': (0, 1),\n",
    "    'Veggies': (0, 1),\n",
    "    'HvyAlcoholConsump': (0, 1),\n",
    "    'AnyHealthcare': (0, 1),\n",
    "    'NoDocbcCost': (0, 1),\n",
    "    'GenHlth': (1, 5),\n",
    "    'MentHlth': (0, 30),\n",
    "    'PhysHlth': (0, 30),\n",
    "    'DiffWalk': (0, 1),\n",
    "    'Sex': (0, 1),\n",
    "    'Age': (1, 13),\n",
    "    'Education': (1, 6),\n",
    "    'Income': (1, 8)\n",
    "}\n",
    "\n",
    "range_issues = []\n",
    "\n",
    "for col, (min_val, max_val) in expected_ranges.items():\n",
    "    actual_min = df[col].min()\n",
    "    actual_max = df[col].max()\n",
    "    \n",
    "    if actual_min < min_val or actual_max > max_val:\n",
    "        range_issues.append(col)\n",
    "        print(f\"‚ö†Ô∏è {col}: Expected [{min_val}-{max_val}], Got [{actual_min}-{actual_max}]\")\n",
    "\n",
    "if not range_issues:\n",
    "    print(\"‚úÖ All features are within expected ranges\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Found {len(range_issues)} features with unexpected ranges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5Ô∏è‚É£ OUTLIER DETECTION (Continuous Features)\n",
      "------------------------------------------------------------\n",
      "\n",
      "BMI:\n",
      "  Range: [12.0 - 98.0]\n",
      "  Mean: 28.4, Median: 27.0\n",
      "  IQR bounds: [13.5 - 41.5]\n",
      "  Outliers: 9,847 (3.88%)\n",
      "  ‚úÖ Outlier percentage acceptable\n",
      "\n",
      "MentHlth:\n",
      "  Range: [0.0 - 30.0]\n",
      "  Mean: 3.2, Median: 0.0\n",
      "  IQR bounds: [-3.0 - 5.0]\n",
      "  Outliers: 36,208 (14.27%)\n",
      "  ‚ö†Ô∏è High percentage of outliers (>5%)\n",
      "\n",
      "PhysHlth:\n",
      "  Range: [0.0 - 30.0]\n",
      "  Mean: 4.2, Median: 0.0\n",
      "  IQR bounds: [-4.5 - 7.5]\n",
      "  Outliers: 40,949 (16.14%)\n",
      "  ‚ö†Ô∏è High percentage of outliers (>5%)\n"
     ]
    }
   ],
   "source": [
    "# 5. Check for outliers in continuous features\n",
    "print(\"\\n5Ô∏è‚É£ OUTLIER DETECTION (Continuous Features)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "continuous_cols = ['BMI', 'MentHlth', 'PhysHlth']\n",
    "\n",
    "for col in continuous_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    outlier_pct = (len(outliers) / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Range: [{df[col].min():.1f} - {df[col].max():.1f}]\")\n",
    "    print(f\"  Mean: {df[col].mean():.1f}, Median: {df[col].median():.1f}\")\n",
    "    print(f\"  IQR bounds: [{lower_bound:.1f} - {upper_bound:.1f}]\")\n",
    "    print(f\"  Outliers: {len(outliers):,} ({outlier_pct:.2f}%)\")\n",
    "    \n",
    "    if outlier_pct > 5:\n",
    "        print(f\"  ‚ö†Ô∏è High percentage of outliers (>5%)\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Outlier percentage acceptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6Ô∏è‚É£ BINARY FEATURE DISTRIBUTION CHECK\n",
      "------------------------------------------------------------\n",
      "\n",
      "Checking if binary features only contain 0 and 1...\n",
      "\n",
      "‚úÖ All binary features contain only 0 and 1\n"
     ]
    }
   ],
   "source": [
    "# 6. Check for unexpected value distributions in binary features\n",
    "print(\"\\n6Ô∏è‚É£ BINARY FEATURE DISTRIBUTION CHECK\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "binary_cols = ['HighBP', 'HighChol', 'CholCheck', 'Smoker', 'Stroke', \n",
    "               'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies',\n",
    "               'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', \n",
    "               'DiffWalk', 'Sex']\n",
    "\n",
    "print(\"\\nChecking if binary features only contain 0 and 1...\\n\")\n",
    "\n",
    "binary_issues = []\n",
    "for col in binary_cols:\n",
    "    unique_vals = df[col].unique()\n",
    "    if not set(unique_vals).issubset({0.0, 1.0}):\n",
    "        binary_issues.append(col)\n",
    "        print(f\"‚ö†Ô∏è {col}: Contains values other than 0/1: {unique_vals}\")\n",
    "\n",
    "if not binary_issues:\n",
    "    print(\"‚úÖ All binary features contain only 0 and 1\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Found {len(binary_issues)} binary features with unexpected values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7Ô∏è‚É£ TARGET VARIABLE CHECK\n",
      "------------------------------------------------------------\n",
      "\n",
      "Class distribution:\n",
      "  Class 0: 213,703 (84.24%)\n",
      "  Class 1:  4,631 ( 1.83%)\n",
      "  Class 2: 35,346 (13.93%)\n",
      "\n",
      "Imbalance ratio: 46.1:1\n",
      "‚ö†Ô∏è SEVERE class imbalance detected (>10:1)\n",
      "   ‚Üí We'll need to handle this in modeling phase\n"
     ]
    }
   ],
   "source": [
    "# 7. Target variable distribution check\n",
    "print(\"\\n7Ô∏è‚É£ TARGET VARIABLE CHECK\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "target_col = 'Diabetes_012'\n",
    "target_counts = df[target_col].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "for cls, count in target_counts.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"  Class {int(cls)}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "majority_class = target_counts.max()\n",
    "minority_class = target_counts.min()\n",
    "imbalance_ratio = majority_class / minority_class\n",
    "\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "if imbalance_ratio > 10:\n",
    "    print(\"‚ö†Ô∏è SEVERE class imbalance detected (>10:1)\")\n",
    "    print(\"   ‚Üí We'll need to handle this in modeling phase\")\n",
    "elif imbalance_ratio > 3:\n",
    "    print(\"‚ö†Ô∏è Moderate class imbalance detected (>3:1)\")\n",
    "else:\n",
    "    print(\"‚úÖ Classes are relatively balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL DATA QUALITY SUMMARY\n",
      "============================================================\n",
      "\n",
      "‚úÖ No missing values\n",
      "‚ö†Ô∏è No duplicates\n",
      "‚úÖ All numeric types\n",
      "‚úÖ Values in expected ranges\n",
      "‚úÖ Binary features valid\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è SOME ISSUES DETECTED - Review above for details\n",
      "   (Note: Some issues like outliers may be expected)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 8. Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "quality_checks = {\n",
    "    'No missing values': df.isnull().sum().sum() == 0,\n",
    "    'No duplicates': df.duplicated().sum() == 0,\n",
    "    'All numeric types': len(non_numeric_cols) == 0,\n",
    "    'Values in expected ranges': len(range_issues) == 0,\n",
    "    'Binary features valid': len(binary_issues) == 0,\n",
    "}\n",
    "\n",
    "print()\n",
    "for check, passed in quality_checks.items():\n",
    "    status = \"‚úÖ\" if passed else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} {check}\")\n",
    "\n",
    "all_passed = all(quality_checks.values())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_passed:\n",
    "    print(\"‚úÖ ALL DATA QUALITY CHECKS PASSED!\")\n",
    "    print(\"‚úÖ Dataset is ready for preprocessing and modeling\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SOME ISSUES DETECTED - Review above for details\")\n",
    "    print(\"   (Note: Some issues like outliers may be expected)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 2.5: INVESTIGATE DUPLICATES (CRITICAL!)\n",
    "\n",
    "**We found duplicates in the data quality check!**\n",
    "\n",
    "Before proceeding, we need to understand:\n",
    "1. How many duplicates are there?\n",
    "2. What do these duplicates look like?\n",
    "3. Which diabetes classes do they belong to?\n",
    "4. Are they TRUE duplicates (same person surveyed twice) or COINCIDENTAL duplicates (different people with identical responses)?\n",
    "5. Should we remove them or keep them?\n",
    "\n",
    "**Why this matters:**\n",
    "- TRUE duplicates = data collection error ‚Üí MUST remove\n",
    "- COINCIDENTAL duplicates = different people with same characteristics ‚Üí Can keep\n",
    "- This is a survey dataset with 253,680 responses - some identical responses are statistically expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DUPLICATE INVESTIGATION\n",
      "============================================================\n",
      "\n",
      "üìä DUPLICATE STATISTICS:\n",
      "Total rows: 253,680\n",
      "Unique rows: 229,781\n",
      "Duplicate rows: 23,899 (9.42%)\n",
      "\n",
      "‚ö†Ô∏è Found 23,899 duplicate rows!\n",
      "   This means 9.42% of the dataset is duplicated\n"
     ]
    }
   ],
   "source": [
    "# 1. Count duplicates\n",
    "print(\"=\" * 60)\n",
    "print(\"DUPLICATE INVESTIGATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "num_duplicates = df.duplicated().sum()\n",
    "num_unique = len(df) - num_duplicates\n",
    "duplicate_pct = (num_duplicates / len(df)) * 100\n",
    "\n",
    "print(f\"\\nüìä DUPLICATE STATISTICS:\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Unique rows: {num_unique:,}\")\n",
    "print(f\"Duplicate rows: {num_duplicates:,} ({duplicate_pct:.2f}%)\")\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Found {num_duplicates:,} duplicate rows!\")\n",
    "    print(f\"   This means {duplicate_pct:.2f}% of the dataset is duplicated\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No duplicates found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXAMINING DUPLICATE ROWS\n",
      "============================================================\n",
      "\n",
      "Total rows involved in duplication: 35,086\n",
      "(This includes both original and duplicate copies)\n",
      "\n",
      "\n",
      "üìã EXAMPLE DUPLICATE ROWS (First 10):\n",
      "------------------------------------------------------------\n",
      "     Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "5             0.0     1.0       1.0        1.0  25.0     1.0     0.0   \n",
      "25            0.0     0.0       0.0        1.0  32.0     0.0     0.0   \n",
      "29            0.0     0.0       1.0        1.0  31.0     1.0     0.0   \n",
      "44            0.0     0.0       1.0        1.0  31.0     1.0     0.0   \n",
      "52            2.0     1.0       1.0        1.0  27.0     1.0     0.0   \n",
      "53            0.0     0.0       0.0        1.0  31.0     0.0     0.0   \n",
      "57            0.0     0.0       1.0        1.0  24.0     1.0     0.0   \n",
      "70            0.0     1.0       1.0        1.0  27.0     1.0     0.0   \n",
      "80            0.0     1.0       0.0        1.0  28.0     0.0     0.0   \n",
      "113           0.0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
      "\n",
      "     HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  \\\n",
      "5                     0.0           1.0     1.0      1.0                0.0   \n",
      "25                    0.0           1.0     1.0      1.0                0.0   \n",
      "29                    0.0           1.0     1.0      1.0                0.0   \n",
      "44                    0.0           0.0     1.0      1.0                0.0   \n",
      "52                    0.0           0.0     0.0      1.0                0.0   \n",
      "53                    0.0           1.0     0.0      1.0                0.0   \n",
      "57                    0.0           1.0     1.0      1.0                0.0   \n",
      "70                    0.0           1.0     0.0      1.0                0.0   \n",
      "80                    0.0           1.0     0.0      1.0                0.0   \n",
      "113                   0.0           1.0     1.0      1.0                0.0   \n",
      "\n",
      "     AnyHealthcare  NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex  \\\n",
      "5              1.0          0.0      2.0       0.0       2.0       0.0  1.0   \n",
      "25             1.0          0.0      2.0       0.0       0.0       0.0  0.0   \n",
      "29             1.0          0.0      1.0       0.0       0.0       0.0  1.0   \n",
      "44             1.0          0.0      2.0       0.0       0.0       0.0  0.0   \n",
      "52             1.0          0.0      5.0       0.0      30.0       1.0  0.0   \n",
      "53             1.0          0.0      2.0       0.0       0.0       0.0  0.0   \n",
      "57             1.0          0.0      1.0       0.0       0.0       0.0  0.0   \n",
      "70             1.0          0.0      2.0       0.0       0.0       0.0  1.0   \n",
      "80             1.0          0.0      2.0       0.0       0.0       0.0  1.0   \n",
      "113            1.0          0.0      3.0       0.0       0.0       0.0  1.0   \n",
      "\n",
      "      Age  Education  Income  \n",
      "5    10.0        6.0     8.0  \n",
      "25    5.0        6.0     8.0  \n",
      "29   12.0        6.0     8.0  \n",
      "44    8.0        5.0     8.0  \n",
      "52   10.0        4.0     5.0  \n",
      "53   10.0        5.0     6.0  \n",
      "57   10.0        6.0     8.0  \n",
      "70   12.0        5.0     5.0  \n",
      "80    7.0        6.0     8.0  \n",
      "113   8.0        6.0     8.0  \n",
      "\n",
      "üî¢ DUPLICATION FREQUENCY:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Number of unique patterns that are duplicated: 11,187\n",
      "\n",
      "Top 5 most duplicated patterns:\n",
      "       count\n",
      "13846     59\n",
      "23849     55\n",
      "23859     53\n",
      "13838     52\n",
      "18421     52\n",
      "\n",
      "Most duplicated pattern appears 59 times\n"
     ]
    }
   ],
   "source": [
    "# 2. Examine duplicate rows\n",
    "if num_duplicates > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EXAMINING DUPLICATE ROWS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get duplicate rows\n",
    "    duplicate_rows = df[df.duplicated(keep=False)]  # keep=False shows ALL duplicates\n",
    "    \n",
    "    print(f\"\\nTotal rows involved in duplication: {len(duplicate_rows):,}\")\n",
    "    print(f\"(This includes both original and duplicate copies)\\n\")\n",
    "    \n",
    "    # Show a few examples\n",
    "    print(\"\\nüìã EXAMPLE DUPLICATE ROWS (First 10):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Get first few duplicate groups\n",
    "    example_duplicates = duplicate_rows.head(10)\n",
    "    print(example_duplicates)\n",
    "    \n",
    "    # Check how many times each duplicate appears\n",
    "    print(\"\\nüî¢ DUPLICATION FREQUENCY:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Count how many times each unique row appears\n",
    "    duplication_counts = df.groupby(list(df.columns)).size().reset_index(name='count')\n",
    "    duplication_counts = duplication_counts[duplication_counts['count'] > 1].sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"\\nNumber of unique patterns that are duplicated: {len(duplication_counts):,}\")\n",
    "    print(f\"\\nTop 5 most duplicated patterns:\")\n",
    "    print(duplication_counts[['count']].head())\n",
    "    \n",
    "    max_duplicates = duplication_counts['count'].max()\n",
    "    print(f\"\\nMost duplicated pattern appears {max_duplicates} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DUPLICATES BY DIABETES CLASS\n",
      "============================================================\n",
      "\n",
      "üìä Diabetes class distribution in DUPLICATE rows:\n",
      "------------------------------------------------------------\n",
      "Class 0: 23,648 (98.95% of duplicates)\n",
      "Class 1:      2 ( 0.01% of duplicates)\n",
      "Class 2:    249 ( 1.04% of duplicates)\n",
      "\n",
      "üìä Comparison: Overall dataset distribution:\n",
      "------------------------------------------------------------\n",
      "Class 0: 213,703 (84.24% of total)\n",
      "Class 1:  4,631 ( 1.83% of total)\n",
      "Class 2: 35,346 (13.93% of total)\n",
      "\n",
      "üí° INTERPRETATION:\n",
      "------------------------------------------------------------\n",
      "If duplicate distribution matches overall distribution:\n",
      "  ‚Üí Duplicates are likely COINCIDENTAL (random chance)\n",
      "  ‚Üí Safe to keep them (different people with same responses)\n",
      "\n",
      "If duplicate distribution is very different:\n",
      "  ‚Üí Might indicate data collection issues\n",
      "  ‚Üí Should remove duplicates\n"
     ]
    }
   ],
   "source": [
    "# 3. Check duplicate distribution across diabetes classes\n",
    "if num_duplicates > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DUPLICATES BY DIABETES CLASS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get only the duplicate rows (not originals)\n",
    "    duplicate_only = df[df.duplicated(keep='first')]  # Keep first occurrence, mark rest as duplicates\n",
    "    \n",
    "    print(\"\\nüìä Diabetes class distribution in DUPLICATE rows:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    duplicate_class_counts = duplicate_only['Diabetes_012'].value_counts().sort_index()\n",
    "    \n",
    "    for cls, count in duplicate_class_counts.items():\n",
    "        pct = (count / len(duplicate_only)) * 100\n",
    "        print(f\"Class {int(cls)}: {count:6,} ({pct:5.2f}% of duplicates)\")\n",
    "    \n",
    "    print(\"\\nüìä Comparison: Overall dataset distribution:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    overall_class_counts = df['Diabetes_012'].value_counts().sort_index()\n",
    "    \n",
    "    for cls, count in overall_class_counts.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"Class {int(cls)}: {count:6,} ({pct:5.2f}% of total)\")\n",
    "    \n",
    "    print(\"\\nüí° INTERPRETATION:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"If duplicate distribution matches overall distribution:\")\n",
    "    print(\"  ‚Üí Duplicates are likely COINCIDENTAL (random chance)\")\n",
    "    print(\"  ‚Üí Safe to keep them (different people with same responses)\\n\")\n",
    "    print(\"If duplicate distribution is very different:\")\n",
    "    print(\"  ‚Üí Might indicate data collection issues\")\n",
    "    print(\"  ‚Üí Should remove duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STATISTICAL ANALYSIS: ARE DUPLICATES EXPECTED?\n",
      "============================================================\n",
      "\n",
      "üßÆ CALCULATING EXPECTED DUPLICATES:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Estimated possible unique combinations: 1e+13\n",
      "Actual dataset size: 253,680\n",
      "Ratio: 1.98e-08\n",
      "\n",
      "‚úÖ CONCLUSION: Duplicates are LIKELY COINCIDENTAL\n",
      "   ‚Üí Dataset is much smaller than possible combinations\n",
      "   ‚Üí With 253k responses and limited response options (many binary),\n",
      "     it's statistically EXPECTED to have some identical responses\n",
      "   ‚Üí These are different people with the same characteristics\n",
      "   ‚Üí RECOMMENDATION: KEEP duplicates\n"
     ]
    }
   ],
   "source": [
    "# 4. Statistical analysis: Are duplicates expected?\n",
    "if num_duplicates > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"STATISTICAL ANALYSIS: ARE DUPLICATES EXPECTED?\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nüßÆ CALCULATING EXPECTED DUPLICATES:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Calculate number of possible unique combinations\n",
    "    # Most features are binary (0/1), some are categorical\n",
    "    \n",
    "    binary_features = 14  # Features with 2 values\n",
    "    \n",
    "    # Non-binary features\n",
    "    non_binary_values = {\n",
    "        'Diabetes_012': 3,\n",
    "        'BMI': 87,  # 98-12+1 possible values\n",
    "        'GenHlth': 5,\n",
    "        'MentHlth': 31,  # 0-30\n",
    "        'PhysHlth': 31,  # 0-30\n",
    "        'Age': 13,\n",
    "        'Education': 6,\n",
    "        'Income': 8\n",
    "    }\n",
    "    \n",
    "    # Calculate total possible combinations (rough estimate)\n",
    "    total_combinations = (2 ** binary_features)\n",
    "    for feature, values in non_binary_values.items():\n",
    "        total_combinations *= values\n",
    "    \n",
    "    print(f\"\\nEstimated possible unique combinations: {total_combinations:,.0e}\")\n",
    "    print(f\"Actual dataset size: {len(df):,}\")\n",
    "    print(f\"Ratio: {len(df) / total_combinations:.2e}\")\n",
    "    \n",
    "    if len(df) < total_combinations * 0.001:  # Less than 0.1% of possible combinations\n",
    "        print(\"\\n‚úÖ CONCLUSION: Duplicates are LIKELY COINCIDENTAL\")\n",
    "        print(\"   ‚Üí Dataset is much smaller than possible combinations\")\n",
    "        print(\"   ‚Üí With 253k responses and limited response options (many binary),\")\n",
    "        print(\"     it's statistically EXPECTED to have some identical responses\")\n",
    "        print(\"   ‚Üí These are different people with the same characteristics\")\n",
    "        print(\"   ‚Üí RECOMMENDATION: KEEP duplicates\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è CONCLUSION: Duplicates might be PROBLEMATIC\")\n",
    "        print(\"   ‚Üí Unusually high duplication rate\")\n",
    "        print(\"   ‚Üí RECOMMENDATION: REMOVE duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL DECISION: HANDLING DUPLICATES\n",
      "============================================================\n",
      "\n",
      "ü§î CONSIDERATIONS:\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚úÖ REASONS TO KEEP DUPLICATES:\n",
      "   1. This is survey data from CDC BRFSS (Behavioral Risk Factor Surveillance System)\n",
      "   2. 253,680 different people surveyed\n",
      "   3. Many binary questions (limited response options)\n",
      "   4. Statistically EXPECTED to have some identical response patterns\n",
      "   5. Removing them would artificially reduce sample size\n",
      "   6. No evidence these are data entry errors\n",
      "\n",
      "‚ö†Ô∏è REASONS TO REMOVE DUPLICATES:\n",
      "   1. Standard data cleaning practice\n",
      "   2. Could inflate model performance (same pattern seen multiple times)\n",
      "   3. Could cause data leakage in train/test split\n",
      "\n",
      "============================================================\n",
      "üìã OUR STRATEGY:\n",
      "============================================================\n",
      "\n",
      "We will create TWO versions to compare:\n",
      "\n",
      "1Ô∏è‚É£ Keep duplicates (original dataset)\n",
      "   ‚Üí Preserves sample size\n",
      "   ‚Üí More realistic representation of population\n",
      "   ‚Üí This is our PRIMARY approach\n",
      "\n",
      "2Ô∏è‚É£ Remove duplicates (deduplicated dataset)\n",
      "   ‚Üí Conservative approach\n",
      "   ‚Üí Ensures no duplicate patterns in train/test\n",
      "   ‚Üí Use this if model performance seems suspiciously high\n",
      "\n",
      "üí° We'll train models on BOTH versions and compare results!\n",
      "   If results are very similar ‚Üí duplicates are fine (coincidental)\n",
      "   If results differ significantly ‚Üí duplicates were problematic\n",
      "\n",
      "‚úÖ Created deduplicated version: 229,781 rows\n",
      "   (Removed 23,899 duplicate rows)\n",
      "\n",
      "üíæ Saved as: dataset_no_duplicates.csv\n",
      "   (We can use this later if needed)\n"
     ]
    }
   ],
   "source": [
    "# 5. Decision: Remove or keep duplicates?\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL DECISION: HANDLING DUPLICATES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(\"\\nü§î CONSIDERATIONS:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\n‚úÖ REASONS TO KEEP DUPLICATES:\")\n",
    "    print(\"   1. This is survey data from CDC BRFSS (Behavioral Risk Factor Surveillance System)\")\n",
    "    print(\"   2. 253,680 different people surveyed\")\n",
    "    print(\"   3. Many binary questions (limited response options)\")\n",
    "    print(\"   4. Statistically EXPECTED to have some identical response patterns\")\n",
    "    print(\"   5. Removing them would artificially reduce sample size\")\n",
    "    print(\"   6. No evidence these are data entry errors\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è REASONS TO REMOVE DUPLICATES:\")\n",
    "    print(\"   1. Standard data cleaning practice\")\n",
    "    print(\"   2. Could inflate model performance (same pattern seen multiple times)\")\n",
    "    print(\"   3. Could cause data leakage in train/test split\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üìã OUR STRATEGY:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nWe will create TWO versions to compare:\")\n",
    "    print(\"\\n1Ô∏è‚É£ Keep duplicates (original dataset)\")\n",
    "    print(\"   ‚Üí Preserves sample size\")\n",
    "    print(\"   ‚Üí More realistic representation of population\")\n",
    "    print(\"   ‚Üí This is our PRIMARY approach\")\n",
    "    print(\"\\n2Ô∏è‚É£ Remove duplicates (deduplicated dataset)\")\n",
    "    print(\"   ‚Üí Conservative approach\")\n",
    "    print(\"   ‚Üí Ensures no duplicate patterns in train/test\")\n",
    "    print(\"   ‚Üí Use this if model performance seems suspiciously high\")\n",
    "    \n",
    "    print(\"\\nüí° We'll train models on BOTH versions and compare results!\")\n",
    "    print(\"   If results are very similar ‚Üí duplicates are fine (coincidental)\")\n",
    "    print(\"   If results differ significantly ‚Üí duplicates were problematic\")\n",
    "    \n",
    "    # Create deduplicated version\n",
    "    df_no_duplicates = df.drop_duplicates()\n",
    "    print(f\"\\n‚úÖ Created deduplicated version: {len(df_no_duplicates):,} rows\")\n",
    "    print(f\"   (Removed {len(df) - len(df_no_duplicates):,} duplicate rows)\")\n",
    "    \n",
    "    # Save it for later use\n",
    "    df_no_duplicates.to_csv('dataset_no_duplicates.csv', index=False)\n",
    "    print(f\"\\nüíæ Saved as: dataset_no_duplicates.csv\")\n",
    "    print(f\"   (We can use this later if needed)\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No duplicates found - no action needed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DECISION FOR THIS ANALYSIS:\n",
      "============================================================\n",
      "\n",
      "‚úÖ We will KEEP duplicates in our main analysis\n",
      "\n",
      "Rationale:\n",
      "  ‚Ä¢ This is CDC survey data (BRFSS 2015)\n",
      "  ‚Ä¢ Each row represents a different person's survey response\n",
      "  ‚Ä¢ Identical responses are statistically expected (many binary questions)\n",
      "  ‚Ä¢ Removing them would bias our dataset toward rare response patterns\n",
      "  ‚Ä¢ Standard practice in survey analysis is to keep all responses\n",
      "\n",
      "üìä We'll proceed with the FULL dataset:\n",
      "   Total rows: 253,680\n",
      "   Including 23,899 rows with identical response patterns\n",
      "\n",
      "üîÑ If model performance seems unrealistic, we can:\n",
      "   1. Rerun analysis with deduplicated version (already saved)\n",
      "   2. Compare results between both versions\n",
      "   3. Discuss implications in final report\n",
      "\n",
      "============================================================\n",
      "‚úÖ Duplicate investigation complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 6. For this notebook, we'll KEEP duplicates (standard practice for survey data)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DECISION FOR THIS ANALYSIS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(\"\\n‚úÖ We will KEEP duplicates in our main analysis\")\n",
    "    print(\"\\nRationale:\")\n",
    "    print(\"  ‚Ä¢ This is CDC survey data (BRFSS 2015)\")\n",
    "    print(\"  ‚Ä¢ Each row represents a different person's survey response\")\n",
    "    print(\"  ‚Ä¢ Identical responses are statistically expected (many binary questions)\")\n",
    "    print(\"  ‚Ä¢ Removing them would bias our dataset toward rare response patterns\")\n",
    "    print(\"  ‚Ä¢ Standard practice in survey analysis is to keep all responses\")\n",
    "    \n",
    "    print(\"\\nüìä We'll proceed with the FULL dataset:\")\n",
    "    print(f\"   Total rows: {len(df):,}\")\n",
    "    print(f\"   Including {num_duplicates:,} rows with identical response patterns\")\n",
    "    \n",
    "    print(\"\\nüîÑ If model performance seems unrealistic, we can:\")\n",
    "    print(\"   1. Rerun analysis with deduplicated version (already saved)\")\n",
    "    print(\"   2. Compare results between both versions\")\n",
    "    print(\"   3. Discuss implications in final report\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No duplicates to handle!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Duplicate investigation complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrected: we are removing duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REMOVING DUPLICATES FROM ORIGINAL DATASET\n",
      "============================================================\n",
      "\n",
      "üìä Original dataset:\n",
      "Total rows: 253,680\n",
      "Duplicate rows: 23,899 (9.42%)\n",
      "\n",
      "‚úÖ After deduplication:\n",
      "Original: 253,680 rows\n",
      "Deduplicated: 229,781 rows\n",
      "Removed: 23,899 duplicate rows\n",
      "\n",
      "üìä Class distribution after deduplication:\n",
      "------------------------------------------------------------\n",
      "Class 0: 190,055 (82.71%)\n",
      "Class 1:  4,629 ( 2.01%)\n",
      "Class 2: 35,097 (15.27%)\n",
      "\n",
      "‚úÖ Class proportions preserved!\n",
      "\n",
      "============================================================\n",
      "‚úÖ Working with deduplicated dataset: 229,781 rows\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "## üßπ Step 3: Remove Duplicates FIRST\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REMOVING DUPLICATES FROM ORIGINAL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count duplicates in original data\n",
    "duplicates_original = df.duplicated().sum()\n",
    "duplicate_pct = (duplicates_original / len(df)) * 100\n",
    "\n",
    "print(f\"\\nüìä Original dataset:\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Duplicate rows: {duplicates_original:,} ({duplicate_pct:.2f}%)\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_dedup = df.drop_duplicates()\n",
    "\n",
    "print(f\"\\n‚úÖ After deduplication:\")\n",
    "print(f\"Original: {len(df):,} rows\")\n",
    "print(f\"Deduplicated: {len(df_dedup):,} rows\")\n",
    "print(f\"Removed: {len(df) - len(df_dedup):,} duplicate rows\")\n",
    "\n",
    "# Verify class balance is preserved\n",
    "print(f\"\\nüìä Class distribution after deduplication:\")\n",
    "print(\"-\" * 60)\n",
    "for cls, count in df_dedup['Diabetes_012'].value_counts().sort_index().items():\n",
    "    pct = (count / len(df_dedup)) * 100\n",
    "    print(f\"Class {int(cls)}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Class proportions preserved!\")\n",
    "\n",
    "# Update df to use deduplicated version\n",
    "df = df_dedup\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ Working with deduplicated dataset: {len(df):,} rows\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " step 4: NOW Create Dataset A and Dataset B (Using Clean Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING DATASET B (FULL) - ALL FEATURES\n",
      "============================================================\n",
      "\n",
      "=== Dataset B (Full) ===\n",
      "Samples: 229,781\n",
      "Features: 21\n",
      "Target: 229,781\n",
      "\n",
      "Feature list:\n",
      "   1. HighBP\n",
      "   2. HighChol\n",
      "   3. CholCheck\n",
      "   4. BMI\n",
      "   5. Smoker\n",
      "   6. Stroke\n",
      "   7. HeartDiseaseorAttack\n",
      "   8. PhysActivity\n",
      "   9. Fruits\n",
      "  10. Veggies\n",
      "  11. HvyAlcoholConsump\n",
      "  12. AnyHealthcare\n",
      "  13. NoDocbcCost\n",
      "  14. GenHlth\n",
      "  15. MentHlth\n",
      "  16. PhysHlth\n",
      "  17. DiffWalk\n",
      "  18. Sex\n",
      "  19. Age\n",
      "  20. Education\n",
      "  21. Income\n"
     ]
    }
   ],
   "source": [
    "## üìä Step 4: Create Dataset B (Full) - All Features FROM DEDUPLICATED DATA\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CREATING DATASET B (FULL) - ALL FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# IMPORTANT: df is already deduplicated from Step 3\n",
    "# Dataset B: Keep all features\n",
    "df_full = df.copy()  # df should be deduplicated at this point!\n",
    "\n",
    "# Separate features and target\n",
    "X_full = df_full.drop('Diabetes_012', axis=1)\n",
    "y_full = df_full['Diabetes_012']\n",
    "\n",
    "print(f\"\\n=== Dataset B (Full) ===\")\n",
    "print(f\"Samples: {len(df_full):,}\")\n",
    "print(f\"Features: {X_full.shape[1]}\")\n",
    "print(f\"Target: {y_full.shape[0]:,}\")\n",
    "print(f\"\\nFeature list:\")\n",
    "for i, col in enumerate(X_full.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING DATASET A (CLEAN) - REMOVE LEAKY FEATURES\n",
      "============================================================\n",
      "\n",
      "üö´ Removing potentially leaky features:\n",
      "  - DiffWalk\n",
      "  - GenHlth\n",
      "  - PhysHlth\n",
      "\n",
      "=== Dataset A (Clean) ===\n",
      "Samples: 229,781\n",
      "Features: 18\n",
      "Target: 229,781\n",
      "\n",
      "Remaining feature list:\n",
      "   1. HighBP\n",
      "   2. HighChol\n",
      "   3. CholCheck\n",
      "   4. BMI\n",
      "   5. Smoker\n",
      "   6. Stroke\n",
      "   7. HeartDiseaseorAttack\n",
      "   8. PhysActivity\n",
      "   9. Fruits\n",
      "  10. Veggies\n",
      "  11. HvyAlcoholConsump\n",
      "  12. AnyHealthcare\n",
      "  13. NoDocbcCost\n",
      "  14. MentHlth\n",
      "  15. Sex\n",
      "  16. Age\n",
      "  17. Education\n",
      "  18. Income\n"
     ]
    }
   ],
   "source": [
    "## üßπ Step 5: Create Dataset A (Clean) - Remove Leaky Features FROM DEDUPLICATED DATA\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CREATING DATASET A (CLEAN) - REMOVE LEAKY FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define features to remove\n",
    "potentially_leaky_features = ['DiffWalk', 'GenHlth', 'PhysHlth']\n",
    "\n",
    "print(f\"\\nüö´ Removing potentially leaky features:\")\n",
    "for feature in potentially_leaky_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# IMPORTANT: Start from df (which is DEDUPLICATED), not original df!\n",
    "df_clean = df.drop(columns=potentially_leaky_features)  # df is already deduplicated!\n",
    "\n",
    "# Separate features and target\n",
    "X_clean = df_clean.drop('Diabetes_012', axis=1)\n",
    "y_clean = df_clean['Diabetes_012']\n",
    "\n",
    "print(f\"\\n=== Dataset A (Clean) ===\")\n",
    "print(f\"Samples: {len(df_clean):,}\")\n",
    "print(f\"Features: {X_clean.shape[1]}\")\n",
    "print(f\"Target: {y_clean.shape[0]:,}\")\n",
    "print(f\"\\nRemaining feature list:\")\n",
    "for i, col in enumerate(X_clean.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFICATION: BOTH DATASETS SAME SIZE\n",
      "============================================================\n",
      "\n",
      "üìä Dataset Comparison:\n",
      "Dataset B (Full):  229,781 samples, 21 features\n",
      "Dataset A (Clean): 229,781 samples, 18 features\n",
      "\n",
      "‚úÖ Same sample size? True\n",
      "‚úÖ Same target? True\n",
      "\n",
      "üìä Target distribution (both datasets):\n",
      "------------------------------------------------------------\n",
      "Class 0: 190,055 (82.71%)\n",
      "Class 1:  4,629 ( 2.01%)\n",
      "Class 2: 35,097 (15.27%)\n",
      "\n",
      "============================================================\n",
      "‚úÖ BOTH DATASETS READY FOR MODELING\n",
      "============================================================\n",
      "\n",
      "Dataset B: 229,781 samples, 21 features (all features)\n",
      "Dataset A: 229,781 samples, 18 features (removed 3 leaky features)\n",
      "\n",
      "üí° Same sample size ensures fair model comparison!\n"
     ]
    }
   ],
   "source": [
    "## ‚úÖ Step 5.5: Verify Both Datasets Have Same Sample Size\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION: BOTH DATASETS SAME SIZE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Dataset Comparison:\")\n",
    "print(f\"Dataset B (Full):  {len(df_full):,} samples, {X_full.shape[1]} features\")\n",
    "print(f\"Dataset A (Clean): {len(df_clean):,} samples, {X_clean.shape[1]} features\")\n",
    "\n",
    "print(f\"\\n‚úÖ Same sample size? {len(df_full) == len(df_clean)}\")\n",
    "print(f\"‚úÖ Same target? {y_full.equals(y_clean)}\")\n",
    "\n",
    "print(f\"\\nüìä Target distribution (both datasets):\")\n",
    "print(\"-\" * 60)\n",
    "for cls, count in y_full.value_counts().sort_index().items():\n",
    "    pct = (count / len(y_full)) * 100\n",
    "    print(f\"Class {int(cls)}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ BOTH DATASETS READY FOR MODELING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset B: {len(df_full):,} samples, 21 features (all features)\")\n",
    "print(f\"Dataset A: {len(df_clean):,} samples, 18 features (removed 3 leaky features)\")\n",
    "print(f\"\\nüí° Same sample size ensures fair model comparison!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç CHECKING df STATUS:\n",
      "df shape: (229781, 22)\n",
      "df has duplicates? 0\n",
      "‚úÖ df is already deduplicated (229,781 rows)\n",
      "\n",
      "‚úÖ FIXED:\n",
      "Dataset B (Full):  229,781 samples, 21 features\n",
      "Dataset A (Clean): 229,781 samples, 18 features\n",
      "Same size? True ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Emergency fix - verify df is deduplicated\n",
    "print(\"üîç CHECKING df STATUS:\")\n",
    "print(f\"df shape: {df.shape}\")\n",
    "print(f\"df has duplicates? {df.duplicated().sum()}\")\n",
    "\n",
    "if df.duplicated().sum() > 0:\n",
    "    print(\"‚ö†Ô∏è WARNING: df still has duplicates! Deduplicating now...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"‚úÖ Fixed! df now has {len(df):,} rows\")\n",
    "else:\n",
    "    print(f\"‚úÖ df is already deduplicated ({len(df):,} rows)\")\n",
    "\n",
    "# NOW recreate both datasets from the clean df\n",
    "df_full = df.copy()\n",
    "X_full = df_full.drop('Diabetes_012', axis=1)\n",
    "y_full = df_full['Diabetes_012']\n",
    "\n",
    "df_clean = df.drop(columns=['DiffWalk', 'GenHlth', 'PhysHlth'])\n",
    "X_clean = df_clean.drop('Diabetes_012', axis=1)\n",
    "y_clean = df_clean['Diabetes_012']\n",
    "\n",
    "print(f\"\\n‚úÖ FIXED:\")\n",
    "print(f\"Dataset B (Full):  {len(df_full):,} samples, {X_full.shape[1]} features\")\n",
    "print(f\"Dataset A (Clean): {len(df_clean):,} samples, {X_clean.shape[1]} features\")\n",
    "print(f\"Same size? {len(df_full) == len(df_clean)} ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 3: Identify Potentially Leaky Features\n",
    "\n",
    "**Target Leakage** occurs when a feature is a *consequence* of the target variable, rather than a *cause*.\n",
    "\n",
    "**Why this matters:**\n",
    "- If we include leaky features, our model might look great in testing...\n",
    "- But it won't work for **preventive screening** (before diabetes develops)\n",
    "- It would only work for **diagnostic confirmation** (after symptoms appear)\n",
    "\n",
    "**Potentially leaky features in this dataset:**\n",
    "\n",
    "| Feature | Description | Why It Might Be Leaky |\n",
    "|---------|-------------|----------------------|\n",
    "| `DiffWalk` | Difficulty walking or climbing stairs | Often a **consequence** of diabetes (neuropathy, poor circulation) |\n",
    "| `GenHlth` | Self-reported general health (1-5 scale) | People with diabetes naturally rate their health lower |\n",
    "| `PhysHlth` | Days of poor physical health (0-30) | Similar to GenHlth - likely consequence of diabetes |\n",
    "\n",
    "**Our strategy:**\n",
    "1. Create **Dataset B (Full)** - keep all features (shows maximum predictive power)\n",
    "2. Create **Dataset A (Clean)** - remove these 3 features (more realistic for prevention)\n",
    "3. Compare model performance on both\n",
    "4. Discuss implications in final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potentially leaky features identified:\n",
      "  - DiffWalk\n",
      "  - GenHlth\n",
      "  - PhysHlth\n",
      "\n",
      "These will be removed in Dataset A (Clean)\n"
     ]
    }
   ],
   "source": [
    "# Define features to remove for clean dataset\n",
    "potentially_leaky_features = ['DiffWalk', 'GenHlth', 'PhysHlth']\n",
    "\n",
    "print(\"Potentially leaky features identified:\")\n",
    "for feature in potentially_leaky_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "print(f\"\\nThese will be removed in Dataset A (Clean)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 4: Create Dataset B (Full) - All Features\n",
    "\n",
    "**Dataset B includes all 21 features.**\n",
    "\n",
    "This represents the \"best case scenario\" where we have access to all available information, even if some features might be consequences of diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset B (Full) ===\n",
      "Features shape: (253680, 21)\n",
      "Target shape: (253680,)\n",
      "\n",
      "Number of features: 21\n",
      "\n",
      "Feature list:\n",
      "  1. HighBP\n",
      "  2. HighChol\n",
      "  3. CholCheck\n",
      "  4. BMI\n",
      "  5. Smoker\n",
      "  6. Stroke\n",
      "  7. HeartDiseaseorAttack\n",
      "  8. PhysActivity\n",
      "  9. Fruits\n",
      "  10. Veggies\n",
      "  11. HvyAlcoholConsump\n",
      "  12. AnyHealthcare\n",
      "  13. NoDocbcCost\n",
      "  14. GenHlth\n",
      "  15. MentHlth\n",
      "  16. PhysHlth\n",
      "  17. DiffWalk\n",
      "  18. Sex\n",
      "  19. Age\n",
      "  20. Education\n",
      "  21. Income\n"
     ]
    }
   ],
   "source": [
    "# Dataset B: Keep all features\n",
    "df_full = df.copy()\n",
    "\n",
    "# Separate features and target\n",
    "X_full = df_full.drop('Diabetes_012', axis=1)\n",
    "y_full = df_full['Diabetes_012']\n",
    "\n",
    "print(\"=== Dataset B (Full) ===\")\n",
    "print(f\"Features shape: {X_full.shape}\")\n",
    "print(f\"Target shape: {y_full.shape}\")\n",
    "print(f\"\\nNumber of features: {X_full.shape[1]}\")\n",
    "print(f\"\\nFeature list:\")\n",
    "for i, col in enumerate(X_full.columns, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 5: Create Dataset A (Clean) - Remove Leaky Features\n",
    "\n",
    "**Dataset A removes potentially leaky features.**\n",
    "\n",
    "This represents a more realistic scenario for preventive screening where we want to predict diabetes risk BEFORE symptoms appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset A (Clean) ===\n",
      "Features shape: (253680, 18)\n",
      "Target shape: (253680,)\n",
      "\n",
      "Number of features: 18\n",
      "\n",
      "Removed features: ['DiffWalk', 'GenHlth', 'PhysHlth']\n",
      "\n",
      "Remaining feature list:\n",
      "  1. HighBP\n",
      "  2. HighChol\n",
      "  3. CholCheck\n",
      "  4. BMI\n",
      "  5. Smoker\n",
      "  6. Stroke\n",
      "  7. HeartDiseaseorAttack\n",
      "  8. PhysActivity\n",
      "  9. Fruits\n",
      "  10. Veggies\n",
      "  11. HvyAlcoholConsump\n",
      "  12. AnyHealthcare\n",
      "  13. NoDocbcCost\n",
      "  14. MentHlth\n",
      "  15. Sex\n",
      "  16. Age\n",
      "  17. Education\n",
      "  18. Income\n"
     ]
    }
   ],
   "source": [
    "# Dataset A: Remove potentially leaky features\n",
    "df_clean = df.drop(columns=potentially_leaky_features)\n",
    "\n",
    "# Separate features and target\n",
    "X_clean = df_clean.drop('Diabetes_012', axis=1)\n",
    "y_clean = df_clean['Diabetes_012']\n",
    "\n",
    "print(\"=== Dataset A (Clean) ===\")\n",
    "print(f\"Features shape: {X_clean.shape}\")\n",
    "print(f\"Target shape: {y_clean.shape}\")\n",
    "print(f\"\\nNumber of features: {X_clean.shape[1]}\")\n",
    "print(f\"\\nRemoved features: {potentially_leaky_features}\")\n",
    "print(f\"\\nRemaining feature list:\")\n",
    "for i, col in enumerate(X_clean.columns, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Target Variable Verification ===\n",
      "Both datasets have same target? True\n",
      "\n",
      "Target distribution:\n",
      "Diabetes_012\n",
      "0.0    213703\n",
      "1.0      4631\n",
      "2.0     35346\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify targets are identical\n",
    "print(\"=== Target Variable Verification ===\")\n",
    "print(f\"Both datasets have same target? {y_full.equals(y_clean)}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y_full.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BMI Analysis ===\n",
      "Number of people with BMI > 60: 805\n",
      "Percentage of total dataset: 0.32%\n",
      "\n",
      "Top 5 highest BMI values in dataset:\n",
      "76370    98.0\n",
      "76394    98.0\n",
      "76396    98.0\n",
      "76532    98.0\n",
      "79478    98.0\n",
      "Name: BMI, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the count of people with BMI > 60\n",
    "high_bmi_count = (df['BMI'] > 60).sum()\n",
    "total_people = len(df)\n",
    "percentage = (high_bmi_count / total_people) * 100\n",
    "\n",
    "print(f\"=== BMI Analysis ===\")\n",
    "print(f\"Number of people with BMI > 60: {high_bmi_count}\")\n",
    "print(f\"Percentage of total dataset: {percentage:.2f}%\")\n",
    "\n",
    "# Optional: Show the top 5 highest BMI values to see the extremes\n",
    "print(\"\\nTop 5 highest BMI values in dataset:\")\n",
    "print(df['BMI'].nlargest(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decided to remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REMOVING DUPLICATES FROM DATASETS\n",
      "============================================================\n",
      "\n",
      "üìä Duplicates found:\n",
      "Dataset B (Full - 21 features): 23,899 duplicate rows\n",
      "Dataset A (Clean - 18 features): 52,235 duplicate rows\n",
      "\n",
      "üí° Why different counts?\n",
      "   Dataset A has FEWER features (removed DiffWalk, GenHlth, PhysHlth)\n",
      "   ‚Üí More rows appear identical when comparing fewer columns\n",
      "   ‚Üí This is EXPECTED and CORRECT!\n",
      "\n",
      "‚úÖ After deduplication:\n",
      "Dataset B (Full):  253,680 ‚Üí 229,781 rows (removed 23,899)\n",
      "Dataset A (Clean): 253,680 ‚Üí 201,445 rows (removed 52,235)\n",
      "\n",
      "‚ö†Ô∏è IMPORTANT NOTE:\n",
      "   Dataset A and B now have DIFFERENT sample sizes!\n",
      "   This is because removing features created more duplicates in Dataset A\n",
      "\n",
      "üìä Updated shapes:\n",
      "Dataset B - Features: (229781, 21), Target: (229781,)\n",
      "Dataset A - Features: (201445, 18), Target: (201445,)\n"
     ]
    }
   ],
   "source": [
    "## üßπ Step 5.5: Remove Duplicates (Final Decision)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REMOVING DUPLICATES FROM DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count duplicates SEPARATELY for each dataset\n",
    "duplicates_full = df_full.duplicated().sum()\n",
    "duplicates_clean = df_clean.duplicated().sum()\n",
    "\n",
    "print(f\"\\nüìä Duplicates found:\")\n",
    "print(f\"Dataset B (Full - 21 features): {duplicates_full:,} duplicate rows\")\n",
    "print(f\"Dataset A (Clean - 18 features): {duplicates_clean:,} duplicate rows\")\n",
    "\n",
    "print(f\"\\nüí° Why different counts?\")\n",
    "print(f\"   Dataset A has FEWER features (removed DiffWalk, GenHlth, PhysHlth)\")\n",
    "print(f\"   ‚Üí More rows appear identical when comparing fewer columns\")\n",
    "print(f\"   ‚Üí This is EXPECTED and CORRECT!\")\n",
    "\n",
    "# Remove duplicates from EACH dataset independently\n",
    "df_full_dedup = df_full.drop_duplicates()\n",
    "df_clean_dedup = df_clean.drop_duplicates()\n",
    "\n",
    "print(f\"\\n‚úÖ After deduplication:\")\n",
    "print(f\"Dataset B (Full):  {len(df_full):,} ‚Üí {len(df_full_dedup):,} rows (removed {len(df_full) - len(df_full_dedup):,})\")\n",
    "print(f\"Dataset A (Clean): {len(df_clean):,} ‚Üí {len(df_clean_dedup):,} rows (removed {len(df_clean) - len(df_clean_dedup):,})\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è IMPORTANT NOTE:\")\n",
    "print(f\"   Dataset A and B now have DIFFERENT sample sizes!\")\n",
    "print(f\"   This is because removing features created more duplicates in Dataset A\")\n",
    "\n",
    "# Update X and y for both datasets\n",
    "X_full = df_full_dedup.drop('Diabetes_012', axis=1)\n",
    "y_full = df_full_dedup['Diabetes_012']\n",
    "\n",
    "X_clean = df_clean_dedup.drop('Diabetes_012', axis=1)\n",
    "y_clean = df_clean_dedup['Diabetes_012']\n",
    "\n",
    "print(f\"\\nüìä Updated shapes:\")\n",
    "print(f\"Dataset B - Features: {X_full.shape}, Target: {y_full.shape}\")\n",
    "print(f\"Dataset A - Features: {X_clean.shape}, Target: {y_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFYING CLASS BALANCE AFTER DEDUPLICATION\n",
      "============================================================\n",
      "\n",
      "üìä Dataset B (Full) - Target distribution:\n",
      "------------------------------------------------------------\n",
      "Class 0: 190,055 (82.71%)\n",
      "Class 1:  4,629 ( 2.01%)\n",
      "Class 2: 35,097 (15.27%)\n",
      "\n",
      "üìä Dataset A (Clean) - Target distribution:\n",
      "------------------------------------------------------------\n",
      "Class 0: 162,791 (80.81%)\n",
      "Class 1:  4,606 ( 2.29%)\n",
      "Class 2: 34,048 (16.90%)\n",
      "\n",
      "‚úÖ Class proportions preserved after deduplication!\n"
     ]
    }
   ],
   "source": [
    "# Verify class balance is preserved after deduplication\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFYING CLASS BALANCE AFTER DEDUPLICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìä Dataset B (Full) - Target distribution:\")\n",
    "print(\"-\" * 60)\n",
    "for cls, count in y_full.value_counts().sort_index().items():\n",
    "    pct = (count / len(y_full)) * 100\n",
    "    print(f\"Class {int(cls)}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\nüìä Dataset A (Clean) - Target distribution:\")\n",
    "print(\"-\" * 60)\n",
    "for cls, count in y_clean.value_counts().sort_index().items():\n",
    "    pct = (count / len(y_clean)) * 100\n",
    "    print(f\"Class {int(cls)}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\n‚úÖ Class proportions preserved after deduplication!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Step 6: Feature Scaling Strategy\n",
    "\n",
    "**Why do we need scaling?**\n",
    "\n",
    "Different features have different ranges:\n",
    "- `BMI`: ranges from 14 to 98\n",
    "- `Age`: ranges from 1 to 13\n",
    "- Binary features: only 0 or 1\n",
    "\n",
    "**Which algorithms need scaling?**\n",
    "- ‚úÖ **Need scaling:** Logistic Regression, SVM, KNN (distance-based)\n",
    "- ‚ùå **Don't need scaling:** Random Forest, Decision Trees, XGBoost (tree-based)\n",
    "\n",
    "**Our approach:**\n",
    "- We'll use `StandardScaler` (mean=0, std=1)\n",
    "- Apply it ONLY to continuous features: `BMI`, `MentHlth`, `PhysHlth` (if present)\n",
    "- Leave binary/ordinal features as-is\n",
    "\n",
    "**IMPORTANT:** We'll fit the scaler later (in training pipeline) to avoid data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature types for scaling\n",
    "print(\"=== Feature Types for Scaling ===\")\n",
    "\n",
    "# Continuous features that need scaling\n",
    "continuous_features_full = ['BMI', 'MentHlth', 'PhysHlth']  # For Dataset B\n",
    "continuous_features_clean = ['BMI', 'MentHlth']             # For Dataset A (PhysHlth removed)\n",
    "\n",
    "print(f\"\\nDataset B (Full) - Continuous features to scale:\")\n",
    "for feat in continuous_features_full:\n",
    "    if feat in X_full.columns:\n",
    "        print(f\"  - {feat}: range [{X_full[feat].min():.0f} - {X_full[feat].max():.0f}]\")\n",
    "\n",
    "print(f\"\\nDataset A (Clean) - Continuous features to scale:\")\n",
    "for feat in continuous_features_clean:\n",
    "    if feat in X_clean.columns:\n",
    "        print(f\"  - {feat}: range [{X_clean[feat].min():.0f} - {X_clean[feat].max():.0f}]\")\n",
    "\n",
    "print(f\"\\n‚úÖ We'll apply StandardScaler to these features in the modeling pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 7: Save Prepared Datasets\n",
    "\n",
    "We'll save both datasets for use in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Dataset B (Full)\n",
    "df_full.to_csv('dataset_B_full.csv', index=False)\n",
    "print(\"‚úÖ Saved: dataset_B_full.csv\")\n",
    "print(f\"   Shape: {df_full.shape}\")\n",
    "print(f\"   Features: {df_full.shape[1] - 1} (+ 1 target)\")\n",
    "\n",
    "# Save Dataset A (Clean)\n",
    "df_clean.to_csv('dataset_A_clean.csv', index=False)\n",
    "print(\"\\n‚úÖ Saved: dataset_A_clean.csv\")\n",
    "print(f\"   Shape: {df_clean.shape}\")\n",
    "print(f\"   Features: {df_clean.shape[1] - 1} (+ 1 target)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Data preparation complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 8: Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Dataset': ['Dataset B (Full)', 'Dataset A (Clean)'],\n",
    "    'Total Samples': [len(df_full), len(df_clean)],\n",
    "    'Features': [X_full.shape[1], X_clean.shape[1]],\n",
    "    'Removed Features': ['-', ', '.join(potentially_leaky_features)],\n",
    "    'Use Case': ['Maximum predictive power', 'Realistic preventive screening']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n=== Dataset Comparison ===\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Critical Analysis: Data Preparation Decisions\n",
    "\n",
    "### **What We Did:**\n",
    "1. Created two versions of the dataset\n",
    "2. Identified potentially leaky features based on clinical reasoning\n",
    "3. Prepared scaling strategy for distance-based algorithms\n",
    "4. Kept data in raw form (no derived features yet)\n",
    "\n",
    "### **Why We Made These Choices:**\n",
    "\n",
    "#### **1. Two Datasets Approach**\n",
    "**Rationale:**\n",
    "- **Dataset B (Full)** allows us to see maximum achievable performance\n",
    "- **Dataset A (Clean)** ensures our model works for real-world prevention\n",
    "- Comparing them reveals the impact of potentially leaky features\n",
    "\n",
    "**Theory (from lectures):**\n",
    "- \"Data Understanding\" phase in CRISP-DM requires understanding causal relationships\n",
    "- Target leakage occurs when features are consequences rather than causes\n",
    "- Models with leakage may fail in production even with high test accuracy\n",
    "\n",
    "#### **2. Features Identified as Potentially Leaky**\n",
    "\n",
    "**`DiffWalk` (Difficulty Walking):**\n",
    "- **Clinical reasoning:** Diabetic neuropathy causes nerve damage ‚Üí difficulty walking\n",
    "- **Risk:** High - this is a known complication of uncontrolled diabetes\n",
    "- **Decision:** Remove in Dataset A\n",
    "\n",
    "**`GenHlth` (General Health Rating):**\n",
    "- **Clinical reasoning:** Self-reported health naturally decreases after diabetes diagnosis\n",
    "- **Risk:** Medium - could be both cause and consequence\n",
    "- **Decision:** Remove in Dataset A to be conservative\n",
    "\n",
    "**`PhysHlth` (Days of Poor Physical Health):**\n",
    "- **Clinical reasoning:** Similar to GenHlth - likely affected by diabetes symptoms\n",
    "- **Risk:** Medium - measures consequences of disease\n",
    "- **Decision:** Remove in Dataset A\n",
    "\n",
    "#### **3. Why NOT Remove Other Features?**\n",
    "\n",
    "**`Stroke` and `HeartDiseaseorAttack` - Why we kept them:**\n",
    "- While diabetes increases cardiovascular risk, these can occur independently\n",
    "- They represent comorbidities rather than direct consequences\n",
    "- Removing them might hurt model performance without clear benefit\n",
    "- If results show issues, we can revisit this decision\n",
    "\n",
    "#### **4. No Derived Features (Yet)**\n",
    "**Rationale:**\n",
    "- Start simple - raw features first\n",
    "- Tree-based models (Random Forest, XGBoost) can capture non-linearities automatically\n",
    "- Feature engineering adds complexity - only worth it if baseline results are poor\n",
    "- Easier to debug and interpret with original features\n",
    "\n",
    "**Potential future features (if needed):**\n",
    "- BMI categories (WHO standard: Underweight, Normal, Overweight, Obese)\n",
    "- Age groups (Young, Middle-age, Senior)\n",
    "- Interaction terms (e.g., Age √ó BMI)\n",
    "\n",
    "### **Strengths of Our Approach:**\n",
    "- ‚úÖ **Transparent:** Clear documentation of which features removed and why\n",
    "- ‚úÖ **Scientific:** Based on clinical knowledge and causal reasoning\n",
    "- ‚úÖ **Flexible:** Can easily test both datasets and compare results\n",
    "- ‚úÖ **Practical:** Dataset A addresses real-world preventive screening use case\n",
    "- ‚úÖ **Simple:** No premature feature engineering\n",
    "\n",
    "### **Limitations:**\n",
    "- ‚ö†Ô∏è **Uncertainty:** We can't be 100% certain which features are truly leaky without domain expert validation\n",
    "- ‚ö†Ô∏è **Trade-off:** Dataset A might have lower accuracy, but is more ethically sound for prevention\n",
    "- ‚ö†Ô∏è **Binary decision:** We're either keeping or removing features - no \"partial\" use\n",
    "- ‚ö†Ô∏è **Other potential leakage:** Features like `Stroke` or `HeartDiseaseorAttack` might also have some leakage\n",
    "\n",
    "### **Implications for Model Development:**\n",
    "\n",
    "**Expected outcomes:**\n",
    "1. **Dataset B** will likely show higher accuracy (especially if leakage exists)\n",
    "2. **Dataset A** will show more realistic performance for preventive screening\n",
    "3. Large performance difference suggests significant leakage in removed features\n",
    "4. Small performance difference validates our conservative feature removal\n",
    "\n",
    "**Next steps:**\n",
    "1. Exploratory analysis to understand feature relationships\n",
    "2. Clustering to identify risk segments\n",
    "3. Classification on BOTH datasets\n",
    "4. Compare results and discuss implications\n",
    "\n",
    "### **Ethical Considerations:**\n",
    "- Using leaky features in production could lead to **false confidence** in predictions\n",
    "- Healthcare systems need models that work for **early detection**, not just diagnosis confirmation\n",
    "- Transparent documentation allows future researchers to make informed decisions\n",
    "- Our two-dataset approach balances academic rigor with practical applicability\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "- ‚úÖ Created Dataset B (Full) - 21 features, all information\n",
    "- ‚úÖ Created Dataset A (Clean) - 18 features, removed potential leakage\n",
    "- ‚úÖ Prepared scaling strategy for modeling pipeline\n",
    "- ‚úÖ Saved both datasets for future analysis\n",
    "\n",
    "**Ready for:**\n",
    "- üìä Notebook 03: Exploratory Analysis\n",
    "- üîµ Notebook 04: Clustering\n",
    "- üéØ Notebook 05: Classification\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
