{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§ª Notebook 05A: Feature Engineering A/B Test\n",
    "\n",
    "**Objective:** Determine if engineered features improve model performance\n",
    "\n",
    "**Critical Question:** Should we use raw features OR raw + engineered features?\n",
    "\n",
    "**Strategy:**\n",
    "- **Set A (Raw):** Original 18 features only\n",
    "- **Set B (Engineered):** Raw + BMI categories + Risk scores + Key interactions\n",
    "- **Test Model:** RandomForest with balanced weights (fast baseline)\n",
    "- **Decision Criteria:** If Set B improves Recall by >2%, use engineered features\n",
    "\n",
    "**Why This Matters:**\n",
    "- No point optimizing models on wrong features\n",
    "- Feature engineering can help OR hurt (overfitting risk)\n",
    "- Must validate with data, not assumptions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    recall_score, precision_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 2: Load Data & Create Binary Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LOADING DATA & DEFINING BINARY TARGET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('dataset_A_clean.csv')\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset loaded: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "\n",
    "# Create binary target (from Phase 1 decision)\n",
    "# Class 0: Healthy (Diabetes_012 == 0)\n",
    "# Class 1: At Risk (Diabetes_012 == 1 OR 2)\n",
    "y = (df['Diabetes_012'] >= 1).astype(int)\n",
    "\n",
    "print(f\"\\nğŸ“Š Binary Target Distribution:\")\n",
    "class_counts = y.value_counts().sort_index()\n",
    "print(f\"   Class 0 (Healthy):  {class_counts[0]:7,} ({class_counts[0]/len(y)*100:.2f}%)\")\n",
    "print(f\"   Class 1 (At Risk):  {class_counts[1]:7,} ({class_counts[1]/len(y)*100:.2f}%)\")\n",
    "print(f\"   Imbalance ratio: {class_counts[0]/class_counts[1]:.1f}:1\")\n",
    "\n",
    "# Store original features (excluding target)\n",
    "X_raw = df.drop('Diabetes_012', axis=1)\n",
    "original_features = X_raw.columns.tolist()\n",
    "\n",
    "print(f\"\\nğŸ“‹ Original Features ({len(original_features)}):\")\n",
    "for i, feat in enumerate(original_features, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 3: Create Feature Set A (Raw Features Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SET A: RAW FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_set_A = X_raw.copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Set A Shape: {X_set_A.shape}\")\n",
    "print(f\"   Features: {X_set_A.shape[1]}\")\n",
    "print(f\"   Samples: {X_set_A.shape[0]:,}\")\n",
    "\n",
    "print(f\"\\nâœ… Set A ready (baseline raw features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Step 4: Create Feature Set B (Raw + Engineered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SET B: RAW + ENGINEERED FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_set_B = X_raw.copy()\n",
    "\n",
    "print(f\"\\nğŸ”§ Engineering new features...\\n\")\n",
    "\n",
    "# ============================================\n",
    "# 1. BMI Categories (clinical thresholds)\n",
    "# ============================================\n",
    "print(\"1ï¸âƒ£ BMI Categories:\")\n",
    "X_set_B['BMI_Underweight'] = (X_set_B['BMI'] < 18.5).astype(int)\n",
    "X_set_B['BMI_Normal'] = ((X_set_B['BMI'] >= 18.5) & (X_set_B['BMI'] < 25)).astype(int)\n",
    "X_set_B['BMI_Overweight'] = ((X_set_B['BMI'] >= 25) & (X_set_B['BMI'] < 30)).astype(int)\n",
    "X_set_B['BMI_Obese'] = (X_set_B['BMI'] >= 30).astype(int)\n",
    "print(f\"   âœ“ Added 4 BMI category indicators\")\n",
    "\n",
    "# ============================================\n",
    "# 2. Metabolic Risk Score (key risk factors)\n",
    "# ============================================\n",
    "print(\"\\n2ï¸âƒ£ Metabolic Risk Score:\")\n",
    "X_set_B['Metabolic_Risk_Score'] = (\n",
    "    X_set_B['HighBP'].astype(int) + \n",
    "    X_set_B['HighChol'].astype(int) + \n",
    "    (X_set_B['BMI'] >= 30).astype(int)  # Obesity\n",
    ")\n",
    "print(f\"   âœ“ Score = HighBP + HighChol + Obesity (0-3 range)\")\n",
    "print(f\"   Distribution:\")\n",
    "print(X_set_B['Metabolic_Risk_Score'].value_counts().sort_index())\n",
    "\n",
    "# ============================================\n",
    "# 3. Cardiovascular History (composite)\n",
    "# ============================================\n",
    "print(\"\\n3ï¸âƒ£ Cardiovascular History:\")\n",
    "X_set_B['Cardiovascular_History'] = (\n",
    "    X_set_B['HeartDiseaseorAttack'].astype(int) + \n",
    "    X_set_B['Stroke'].astype(int)\n",
    ")\n",
    "print(f\"   âœ“ Any heart disease or stroke history\")\n",
    "\n",
    "# ============================================\n",
    "# 4. Age-BMI Interaction (critical for diabetes)\n",
    "# ============================================\n",
    "print(\"\\n4ï¸âƒ£ Age Ã— BMI Interaction:\")\n",
    "X_set_B['Age_BMI_Interaction'] = X_set_B['Age'] * X_set_B['BMI']\n",
    "print(f\"   âœ“ Captures compound risk (older + heavier = higher risk)\")\n",
    "\n",
    "# ============================================\n",
    "# 5. Lifestyle Health Score (protective factors)\n",
    "# ============================================\n",
    "print(\"\\n5ï¸âƒ£ Lifestyle Health Score:\")\n",
    "X_set_B['Lifestyle_Health_Score'] = (\n",
    "    X_set_B['PhysActivity'].astype(int) + \n",
    "    X_set_B['Fruits'].astype(int) + \n",
    "    X_set_B['Veggies'].astype(int) + \n",
    "    (1 - X_set_B['Smoker'].astype(int))  # Non-smoker is positive\n",
    ")\n",
    "print(f\"   âœ“ Healthy behaviors score (0-4 range)\")\n",
    "\n",
    "# ============================================\n",
    "# 6. Mental Health Risk Flag\n",
    "# ============================================\n",
    "print(\"\\n6ï¸âƒ£ Mental Health Risk:\")\n",
    "X_set_B['MentHlth_Risk'] = (X_set_B['MentHlth'] >= 15).astype(int)  # >15 days = risk\n",
    "print(f\"   âœ“ Flag for poor mental health (â‰¥15 days)\")\n",
    "\n",
    "# ============================================\n",
    "# 7. Healthcare Access Score\n",
    "# ============================================\n",
    "print(\"\\n7ï¸âƒ£ Healthcare Access:\")\n",
    "X_set_B['Healthcare_Access'] = (\n",
    "    X_set_B['AnyHealthcare'].astype(int) + \n",
    "    X_set_B['CholCheck'].astype(int)\n",
    ")\n",
    "print(f\"   âœ“ Access to care + preventive screening\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"ğŸ“Š Set B Shape: {X_set_B.shape}\")\n",
    "print(f\"   Original features: {len(original_features)}\")\n",
    "print(f\"   Engineered features: {X_set_B.shape[1] - len(original_features)}\")\n",
    "print(f\"   Total features: {X_set_B.shape[1]}\")\n",
    "\n",
    "# List new features\n",
    "new_features = [col for col in X_set_B.columns if col not in original_features]\n",
    "print(f\"\\nğŸ“‹ Engineered Features ({len(new_features)}):\")\n",
    "for i, feat in enumerate(new_features, 1):\n",
    "    print(f\"   {i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\nâœ… Set B ready (raw + engineered features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”€ Step 5: Train-Test Split (Same for Both Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CREATING TRAIN-TEST SPLITS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Split Set A\n",
    "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(\n",
    "    X_set_A, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split Set B (same random_state = same split)\n",
    "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(\n",
    "    X_set_B, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š Set A:\")\n",
    "print(f\"   Train: {X_train_A.shape[0]:,} samples Ã— {X_train_A.shape[1]} features\")\n",
    "print(f\"   Test:  {X_test_A.shape[0]:,} samples Ã— {X_test_A.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Set B:\")\n",
    "print(f\"   Train: {X_train_B.shape[0]:,} samples Ã— {X_train_B.shape[1]} features\")\n",
    "print(f\"   Test:  {X_test_B.shape[0]:,} samples Ã— {X_test_B.shape[1]} features\")\n",
    "\n",
    "# Verify same samples\n",
    "print(f\"\\nâœ… Both sets use SAME train/test split (random_state=42)\")\n",
    "print(f\"   This ensures fair comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 6: Test Set A (Raw Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TESTING SET A: RAW FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define pipeline\n",
    "pipeline_A = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train\n",
    "print(f\"\\nâ³ Training Random Forest on Set A...\")\n",
    "pipeline_A.fit(X_train_A, y_train_A)\n",
    "print(f\"âœ… Training complete\")\n",
    "\n",
    "# Predict\n",
    "y_pred_A = pipeline_A.predict(X_test_A)\n",
    "y_pred_proba_A = pipeline_A.predict_proba(X_test_A)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "recall_A = recall_score(y_test_A, y_pred_A)\n",
    "precision_A = precision_score(y_test_A, y_pred_A)\n",
    "f1_A = f1_score(y_test_A, y_pred_A)\n",
    "roc_auc_A = roc_auc_score(y_test_A, y_pred_proba_A)\n",
    "\n",
    "print(f\"\\nğŸ“Š Set A Performance:\")\n",
    "print(f\"   Recall:    {recall_A:.3f} â† PRIMARY METRIC\")\n",
    "print(f\"   Precision: {precision_A:.3f}\")\n",
    "print(f\"   F1-Score:  {f1_A:.3f}\")\n",
    "print(f\"   ROC-AUC:   {roc_auc_A:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_A = confusion_matrix(y_test_A, y_pred_A)\n",
    "tn_A, fp_A, fn_A, tp_A = cm_A.ravel()\n",
    "\n",
    "print(f\"\\nğŸ“Š Confusion Matrix:\")\n",
    "print(f\"   True Negatives:  {tn_A:6,}\")\n",
    "print(f\"   False Positives: {fp_A:6,}\")\n",
    "print(f\"   False Negatives: {fn_A:6,} â† Missed At-Risk\")\n",
    "print(f\"   True Positives:  {tp_A:6,} â† Caught At-Risk\")\n",
    "print(f\"\\n   Catches {tp_A:,} out of {tp_A + fn_A:,} At-Risk patients ({recall_A*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 7: Test Set B (Raw + Engineered Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TESTING SET B: RAW + ENGINEERED FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define pipeline\n",
    "pipeline_B = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train\n",
    "print(f\"\\nâ³ Training Random Forest on Set B...\")\n",
    "pipeline_B.fit(X_train_B, y_train_B)\n",
    "print(f\"âœ… Training complete\")\n",
    "\n",
    "# Predict\n",
    "y_pred_B = pipeline_B.predict(X_test_B)\n",
    "y_pred_proba_B = pipeline_B.predict_proba(X_test_B)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "recall_B = recall_score(y_test_B, y_pred_B)\n",
    "precision_B = precision_score(y_test_B, y_pred_B)\n",
    "f1_B = f1_score(y_test_B, y_pred_B)\n",
    "roc_auc_B = roc_auc_score(y_test_B, y_pred_proba_B)\n",
    "\n",
    "print(f\"\\nğŸ“Š Set B Performance:\")\n",
    "print(f\"   Recall:    {recall_B:.3f} â† PRIMARY METRIC\")\n",
    "print(f\"   Precision: {precision_B:.3f}\")\n",
    "print(f\"   F1-Score:  {f1_B:.3f}\")\n",
    "print(f\"   ROC-AUC:   {roc_auc_B:.3f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_B = confusion_matrix(y_test_B, y_pred_B)\n",
    "tn_B, fp_B, fn_B, tp_B = cm_B.ravel()\n",
    "\n",
    "print(f\"\\nğŸ“Š Confusion Matrix:\")\n",
    "print(f\"   True Negatives:  {tn_B:6,}\")\n",
    "print(f\"   False Positives: {fp_B:6,}\")\n",
    "print(f\"   False Negatives: {fn_B:6,} â† Missed At-Risk\")\n",
    "print(f\"   True Positives:  {tp_B:6,} â† Caught At-Risk\")\n",
    "print(f\"\\n   Catches {tp_B:,} out of {tp_B + fn_B:,} At-Risk patients ({recall_B*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 8: Direct Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"A/B TEST RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Feature Set': 'Set A (Raw)',\n",
    "        'N_Features': X_set_A.shape[1],\n",
    "        'Recall': recall_A,\n",
    "        'Precision': precision_A,\n",
    "        'F1': f1_A,\n",
    "        'ROC_AUC': roc_auc_A\n",
    "    },\n",
    "    {\n",
    "        'Feature Set': 'Set B (Engineered)',\n",
    "        'N_Features': X_set_B.shape[1],\n",
    "        'Recall': recall_B,\n",
    "        'Precision': precision_B,\n",
    "        'F1': f1_B,\n",
    "        'ROC_AUC': roc_auc_B\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\nğŸ“Š Performance Comparison:\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Calculate deltas\n",
    "recall_improvement = recall_B - recall_A\n",
    "recall_improvement_pct = (recall_improvement / recall_A) * 100\n",
    "roc_improvement = roc_auc_B - roc_auc_A\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"IMPROVEMENT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Recall (Primary Metric):\")\n",
    "print(f\"   Set A: {recall_A:.3f}\")\n",
    "print(f\"   Set B: {recall_B:.3f}\")\n",
    "print(f\"   Î” = {recall_improvement:+.3f} ({recall_improvement_pct:+.1f}%)\")\n",
    "\n",
    "if recall_improvement > 0:\n",
    "    print(f\"   âœ… Set B catches {int(recall_improvement * (tp_B + fn_B))} MORE At-Risk patients\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ Set B catches {int(abs(recall_improvement) * (tp_B + fn_B))} FEWER At-Risk patients\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ROC-AUC (Secondary Metric):\")\n",
    "print(f\"   Set A: {roc_auc_A:.3f}\")\n",
    "print(f\"   Set B: {roc_auc_B:.3f}\")\n",
    "print(f\"   Î” = {roc_improvement:+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 9: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison charts\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Recall comparison\n",
    "sets = ['Set A\\n(Raw)', 'Set B\\n(Engineered)']\n",
    "recalls = [recall_A, recall_B]\n",
    "colors = ['steelblue', 'coral']\n",
    "\n",
    "axes[0].bar(sets, recalls, color=colors, alpha=0.7)\n",
    "axes[0].set_ylabel('Recall (At-Risk)', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Recall Comparison\\n(Higher is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].axhline(y=0.70, color='green', linestyle='--', alpha=0.5, label='Target (70%)')\n",
    "axes[0].legend()\n",
    "for i, v in enumerate(recalls):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 2. Precision comparison\n",
    "precisions = [precision_A, precision_B]\n",
    "axes[1].bar(sets, precisions, color=colors, alpha=0.7)\n",
    "axes[1].set_ylabel('Precision (At-Risk)', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('Precision Comparison\\n(Higher is Better)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylim(0, 1)\n",
    "for i, v in enumerate(precisions):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 3. ROC-AUC comparison\n",
    "roc_aucs = [roc_auc_A, roc_auc_B]\n",
    "axes[2].bar(sets, roc_aucs, color=colors, alpha=0.7)\n",
    "axes[2].set_ylabel('ROC-AUC', fontsize=11, fontweight='bold')\n",
    "axes[2].set_title('ROC-AUC Comparison\\n(Higher is Better)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].axhline(y=0.5, color='red', linestyle='--', alpha=0.3, label='Random (0.5)')\n",
    "axes[2].legend()\n",
    "for i, v in enumerate(roc_aucs):\n",
    "    axes[2].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 10: Decision Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DECISION GATE: FEATURE ENGINEERING A/B TEST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Decision criteria\n",
    "recall_threshold = 0.02  # 2% improvement required\n",
    "\n",
    "print(f\"\\nğŸ“‹ Decision Criteria:\")\n",
    "print(f\"   If Set B improves Recall by >{recall_threshold:.1%}, use engineered features\")\n",
    "print(f\"   Otherwise, use raw features (simpler is better if no gain)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Actual Results:\")\n",
    "print(f\"   Recall improvement: {recall_improvement:+.3f} ({recall_improvement_pct:+.1f}%)\")\n",
    "\n",
    "# Make decision\n",
    "if recall_improvement > recall_threshold:\n",
    "    decision = \"SET_B\"\n",
    "    selected_features = X_set_B\n",
    "    selected_feature_names = X_set_B.columns.tolist()\n",
    "    \n",
    "    print(f\"\\nâœ… DECISION: Use SET B (Engineered Features)\")\n",
    "    print(f\"\\nğŸ“‹ Reasoning:\")\n",
    "    print(f\"   â€¢ Recall improved by {recall_improvement:.3f} (>{recall_threshold:.2f} threshold)\")\n",
    "    print(f\"   â€¢ Catches {int(recall_improvement * (tp_B + fn_B))} MORE At-Risk patients\")\n",
    "    print(f\"   â€¢ ROC-AUC also improved by {roc_improvement:+.3f}\")\n",
    "    print(f\"   â€¢ Feature engineering provides measurable value\")\n",
    "    \n",
    "elif recall_improvement > 0:\n",
    "    decision = \"SET_A\"\n",
    "    selected_features = X_set_A\n",
    "    selected_feature_names = X_set_A.columns.tolist()\n",
    "    \n",
    "    print(f\"\\nâš ï¸ DECISION: Use SET A (Raw Features)\")\n",
    "    print(f\"\\nğŸ“‹ Reasoning:\")\n",
    "    print(f\"   â€¢ Recall improved by only {recall_improvement:.3f} (<{recall_threshold:.2f} threshold)\")\n",
    "    print(f\"   â€¢ Improvement too small to justify added complexity\")\n",
    "    print(f\"   â€¢ Simpler model (fewer features) is preferable\")\n",
    "    print(f\"   â€¢ Risk of overfitting with engineered features\")\n",
    "    \n",
    "else:\n",
    "    decision = \"SET_A\"\n",
    "    selected_features = X_set_A\n",
    "    selected_feature_names = X_set_A.columns.tolist()\n",
    "    \n",
    "    print(f\"\\nâŒ DECISION: Use SET A (Raw Features)\")\n",
    "    print(f\"\\nğŸ“‹ Reasoning:\")\n",
    "    print(f\"   â€¢ Recall DECREASED by {abs(recall_improvement):.3f}\")\n",
    "    print(f\"   â€¢ Engineered features HURT performance\")\n",
    "    print(f\"   â€¢ Likely introduced noise or overfitting\")\n",
    "    print(f\"   â€¢ Stick with raw features\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Selected Feature Set:\")\n",
    "print(f\"   Number of features: {len(selected_feature_names)}\")\n",
    "print(f\"   Baseline recall: {recall_B if decision == 'SET_B' else recall_A:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 11: Save Decision for Next Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SAVING RESULTS FOR PHASE 2\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save selected features\n",
    "selected_features_df = selected_features.copy()\n",
    "selected_features_df['Target'] = y\n",
    "selected_features_df.to_csv('selected_features_05A.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ… Saved: selected_features_05A.csv\")\n",
    "print(f\"   Shape: {selected_features_df.shape}\")\n",
    "\n",
    "# Save decision metadata\n",
    "import json\n",
    "decision_data = {\n",
    "    'decision': decision,\n",
    "    'feature_count': len(selected_feature_names),\n",
    "    'feature_names': selected_feature_names,\n",
    "    'set_A_recall': float(recall_A),\n",
    "    'set_B_recall': float(recall_B),\n",
    "    'recall_improvement': float(recall_improvement),\n",
    "    'set_A_roc_auc': float(roc_auc_A),\n",
    "    'set_B_roc_auc': float(roc_auc_B),\n",
    "    'baseline_recall': float(recall_B if decision == 'SET_B' else recall_A)\n",
    "}\n",
    "\n",
    "with open('feature_engineering_decision.json', 'w') as f:\n",
    "    json.dump(decision_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Saved: feature_engineering_decision.json\")\n",
    "print(f\"   Decision: {decision}\")\n",
    "print(f\"   Baseline Recall: {decision_data['baseline_recall']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ¯ PHASE 1 COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nâœ… Ready for Phase 2: Model Tournament\")\n",
    "print(f\"   Next notebook: 05B_model_tournament.ipynb\")\n",
    "print(f\"   Will test 4 models on {decision} features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
