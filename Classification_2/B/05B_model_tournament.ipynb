{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfc6 Notebook 05B: Model Tournament (4 Algorithms)\n",
    "\n",
    "**Objective:** Select top 2 models for Phase 3 sampling optimization\n",
    "\n",
    "**Critical Questions:**\n",
    "1. Which algorithms handle 4.8:1 class imbalance best?\n",
    "2. Do boosting methods outperform bagging (Random Forest)?\n",
    "3. Which 2 models should we optimize in Phase 3?\n",
    "\n",
    "**Models to Test:**\n",
    "1. **Random Forest** - Bagging ensemble (baseline: 21.0% recall)\n",
    "2. **XGBoost** - Gradient boosting (often best for tabular data)\n",
    "3. **AdaBoost** - Adaptive boosting (sequential error correction)\n",
    "4. **CatBoost** - Category-optimized gradient boosting\n",
    "\n",
    "**Configuration:**\n",
    "- Default hyperparameters (NO tuning)\n",
    "- Class imbalance handling (balanced weights / scale_pos_weight)\n",
    "- 3-fold stratified cross-validation\n",
    "- Same features (18 raw features from Phase 1)\n",
    "\n",
    "**Selection Criteria:**\n",
    "- **Primary:** Recall (catch At-Risk patients)\n",
    "- **Secondary:** ROC-AUC (overall discrimination)\n",
    "- **Tiebreaker:** Training time\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, recall_score, precision_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"\u2705 Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Step 2: Load Selected Features from Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LOADING DATA FROM PHASE 1 DECISION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load selected features (Phase 1 output)\n",
    "df = pd.read_csv('selected_features_05A.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Dataset: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "print(f\"\\n\ud83d\udccb Selected Features ({X.shape[1]}):\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "# Check target distribution\n",
    "class_counts = y.value_counts().sort_index()\n",
    "print(f\"\\n\ud83d\udcca Target Distribution:\")\n",
    "print(f\"   Class 0 (Healthy):  {class_counts[0]:7,} ({class_counts[0]/len(y)*100:.2f}%)\")\n",
    "print(f\"   Class 1 (At Risk):  {class_counts[1]:7,} ({class_counts[1]/len(y)*100:.2f}%)\")\n",
    "print(f\"   Imbalance ratio: {class_counts[0]/class_counts[1]:.1f}:1\")\n",
    "\n",
    "# Load Phase 1 decision metadata\n",
    "with open('feature_engineering_decision.json', 'r') as f:\n",
    "    phase1_decision = json.load(f)\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Phase 1 Baseline (Random Forest):\")\n",
    "print(f\"   Recall: {phase1_decision['baseline_recall']:.3f}\")\n",
    "print(f\"   \u2192 This is the benchmark to beat!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd00 Step 3: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CREATING TRAIN-TEST SPLIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Split:\")\n",
    "print(f\"   Train: {X_train.shape[0]:,} samples\")\n",
    "print(f\"   Test:  {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Verify stratification\n",
    "train_dist = y_train.value_counts(normalize=True).sort_index()\n",
    "test_dist = y_test.value_counts(normalize=True).sort_index()\n",
    "\n",
    "print(f\"\\n\u2705 Stratification verified:\")\n",
    "print(f\"   Train At-Risk: {train_dist[1]:.3f}\")\n",
    "print(f\"   Test At-Risk:  {test_dist[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\udd16 Step 4: Define 4 Models with Default Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DEFINING 4 MODELS (DEFAULT PARAMS + BALANCED WEIGHTS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate scale_pos_weight for XGBoost/CatBoost\n",
    "scale_pos_weight = class_counts[0] / class_counts[1]\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=100,        # Default\n",
    "            class_weight='balanced', # Handle imbalance\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'XGBoost': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', XGBClassifier(\n",
    "            n_estimators=100,                      # Default\n",
    "            scale_pos_weight=scale_pos_weight,     # Handle imbalance\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            eval_metric='logloss'                  # Suppress warning\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'AdaBoost': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', AdaBoostClassifier(\n",
    "            n_estimators=100,        # Default\n",
    "            algorithm='SAMME',       # Works with class_weight\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "    \n",
    "    'CatBoost': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', CatBoostClassifier(\n",
    "            iterations=100,                  # Default\n",
    "            auto_class_weights='Balanced',   # Handle imbalance\n",
    "            random_state=42,\n",
    "            verbose=0                        # Suppress output\n",
    "        ))\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Models defined:\")\n",
    "print(f\"   1. Random Forest (Bagging ensemble)\")\n",
    "print(f\"   2. XGBoost (Gradient boosting)\")\n",
    "print(f\"   3. AdaBoost (Adaptive boosting)\")\n",
    "print(f\"   4. CatBoost (Categorical boosting)\")\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 All models use:\")\n",
    "print(f\"   \u2022 Default hyperparameters (no tuning)\")\n",
    "print(f\"   \u2022 Balanced class weights (imbalance handling)\")\n",
    "print(f\"   \u2022 StandardScaler (feature normalization)\")\n",
    "print(f\"   \u2022 n_estimators=100 (fair comparison)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\uddea Step 5: Cross-Validation (3-Fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RUNNING 3-FOLD CROSS-VALIDATION ON ALL MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score, needs_proba=True)\n",
    "}\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "print(f\"\\n\u23f3 Running cross-validation (this may take 5-10 minutes)...\\n\")\n",
    "\n",
    "for model_name, pipeline in models.items():\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Testing: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run cross-validation\n",
    "    cv_scores = cross_validate(\n",
    "        pipeline, X_train, y_train,\n",
    "        cv=3,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate mean and std for each metric\n",
    "    recall_mean = cv_scores['test_recall'].mean()\n",
    "    recall_std = cv_scores['test_recall'].std()\n",
    "    precision_mean = cv_scores['test_precision'].mean()\n",
    "    f1_mean = cv_scores['test_f1'].mean()\n",
    "    roc_auc_mean = cv_scores['test_roc_auc'].mean()\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Cross-Validation Results (3 folds):\")\n",
    "    print(f\"   Recall:    {recall_mean:.3f} \u00b1 {recall_std:.3f} \u2190 PRIMARY\")\n",
    "    print(f\"   Precision: {precision_mean:.3f}\")\n",
    "    print(f\"   F1-Score:  {f1_mean:.3f}\")\n",
    "    print(f\"   ROC-AUC:   {roc_auc_mean:.3f}\")\n",
    "    print(f\"   Time:      {elapsed_time:.1f} seconds\")\n",
    "    \n",
    "    # Store results\n",
    "    cv_results[model_name] = {\n",
    "        'recall_mean': recall_mean,\n",
    "        'recall_std': recall_std,\n",
    "        'precision_mean': precision_mean,\n",
    "        'f1_mean': f1_mean,\n",
    "        'roc_auc_mean': roc_auc_mean,\n",
    "        'time': elapsed_time,\n",
    "        'recall_scores': cv_scores['test_recall']\n",
    "    }\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"\u2705 CROSS-VALIDATION COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Step 6: Comparison & Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL TOURNAMENT RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': model_name,\n",
    "        'Recall_Mean': results['recall_mean'],\n",
    "        'Recall_Std': results['recall_std'],\n",
    "        'Precision': results['precision_mean'],\n",
    "        'F1': results['f1_mean'],\n",
    "        'ROC_AUC': results['roc_auc_mean'],\n",
    "        'Time_Sec': results['time']\n",
    "    }\n",
    "    for model_name, results in cv_results.items()\n",
    "]).sort_values('Recall_Mean', ascending=False)\n",
    "\n",
    "print(\"\\n\ud83d\udcca Ranked by Recall (Primary Metric):\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify top 2\n",
    "top_2 = comparison_df.head(2)\n",
    "winner = top_2.iloc[0]\n",
    "runner_up = top_2.iloc[1]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOP 2 MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n\ud83e\udd47 WINNER: {winner['Model']}\")\n",
    "print(f\"   Recall: {winner['Recall_Mean']:.3f} \u00b1 {winner['Recall_Std']:.3f}\")\n",
    "print(f\"   Precision: {winner['Precision']:.3f}\")\n",
    "print(f\"   F1-Score: {winner['F1']:.3f}\")\n",
    "print(f\"   ROC-AUC: {winner['ROC_AUC']:.3f}\")\n",
    "\n",
    "print(f\"\\n\ud83e\udd48 RUNNER-UP: {runner_up['Model']}\")\n",
    "print(f\"   Recall: {runner_up['Recall_Mean']:.3f} \u00b1 {runner_up['Recall_Std']:.3f}\")\n",
    "print(f\"   Gap: {winner['Recall_Mean'] - runner_up['Recall_Mean']:.3f} ({(winner['Recall_Mean'] - runner_up['Recall_Mean'])*100:.1f} percentage points)\")\n",
    "\n",
    "# Compare to Phase 1 baseline\n",
    "baseline_recall = phase1_decision['baseline_recall']\n",
    "winner_improvement = winner['Recall_Mean'] - baseline_recall\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Improvement over Phase 1 Baseline:\")\n",
    "print(f\"   Baseline (Random Forest, single run): {baseline_recall:.3f}\")\n",
    "print(f\"   Winner ({winner['Model']}, 3-fold CV): {winner['Recall_Mean']:.3f}\")\n",
    "print(f\"   Improvement: {winner_improvement:+.3f} ({winner_improvement/baseline_recall*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Step 7: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "models_list = comparison_df['Model'].tolist()\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12']\n",
    "\n",
    "# 1. Recall with error bars\n",
    "axes[0, 0].barh(models_list, comparison_df['Recall_Mean'], xerr=comparison_df['Recall_Std'],\n",
    "               color=colors, capsize=5, alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Recall (At-Risk)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('Recall with Standard Deviation\\n(Higher is Better)', \n",
    "                    fontsize=13, fontweight='bold')\n",
    "axes[0, 0].axvline(x=0.70, color='green', linestyle='--', alpha=0.5, label='Target (70%)')\n",
    "axes[0, 0].axvline(x=baseline_recall, color='red', linestyle='--', alpha=0.5, \n",
    "                  label=f'Baseline ({baseline_recall:.3f})')\n",
    "axes[0, 0].legend()\n",
    "for i, (v, std) in enumerate(zip(comparison_df['Recall_Mean'], comparison_df['Recall_Std'])):\n",
    "    axes[0, 0].text(v + std + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "# 2. Precision\n",
    "axes[0, 1].barh(models_list, comparison_df['Precision'], color=colors, alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Precision (At-Risk)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Precision\\n(Higher is Better)', fontsize=13, fontweight='bold')\n",
    "for i, v in enumerate(comparison_df['Precision']):\n",
    "    axes[0, 1].text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "# 3. F1-Score\n",
    "axes[1, 0].barh(models_list, comparison_df['F1'], color=colors, alpha=0.7)\n",
    "axes[1, 0].set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('F1-Score (Harmonic Mean)\\n(Higher is Better)', \n",
    "                    fontsize=13, fontweight='bold')\n",
    "for i, v in enumerate(comparison_df['F1']):\n",
    "    axes[1, 0].text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "# 4. ROC-AUC\n",
    "axes[1, 1].barh(models_list, comparison_df['ROC_AUC'], color=colors, alpha=0.7)\n",
    "axes[1, 1].set_xlabel('ROC-AUC', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('ROC-AUC Score\\n(Higher is Better)', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].axvline(x=0.5, color='red', linestyle='--', alpha=0.3, label='Random (0.5)')\n",
    "axes[1, 1].legend()\n",
    "for i, v in enumerate(comparison_df['ROC_AUC']):\n",
    "    axes[1, 1].text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d Step 8: Critical Analysis - Why These Results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CRITICAL ANALYSIS: WHY DID EACH MODEL PERFORM THIS WAY?\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, row in comparison_df.iterrows():\n",
    "    model_name = row['Model']\n",
    "    recall = row['Recall_Mean']\n",
    "    rank = list(comparison_df['Model']).index(model_name) + 1\n",
    "    \n",
    "    rank_emoji = {1: '\ud83e\udd47', 2: '\ud83e\udd48', 3: '\ud83e\udd49', 4: '4\ufe0f\u20e3'}[rank]\n",
    "    \n",
    "    print(f\"\\n{rank_emoji} {model_name} (Recall: {recall:.3f})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if model_name == 'Random Forest':\n",
    "        if recall > 0.25:\n",
    "            print(\"\u2705 Strengths:\")\n",
    "            print(\"   \u2022 Bagging ensemble reduces variance\")\n",
    "            print(\"   \u2022 Handles non-linear relationships well\")\n",
    "            print(\"   \u2022 Balanced class weights work effectively\")\n",
    "            print(\"   \u2022 Robust to overfitting with 100 trees\")\n",
    "        else:\n",
    "            print(\"\u26a0\ufe0f Limitations:\")\n",
    "            print(\"   \u2022 Each tree trained independently (no sequential learning)\")\n",
    "            print(\"   \u2022 May not capture complex patterns as well as boosting\")\n",
    "            print(\"   \u2022 Default max_depth may be too shallow\")\n",
    "    \n",
    "    elif model_name == 'XGBoost':\n",
    "        if recall > 0.25:\n",
    "            print(\"\u2705 Strengths:\")\n",
    "            print(\"   \u2022 Sequential tree building corrects previous errors\")\n",
    "            print(\"   \u2022 scale_pos_weight handles imbalance effectively\")\n",
    "            print(\"   \u2022 Regularization prevents overfitting\")\n",
    "            print(\"   \u2022 Often best-in-class for tabular data\")\n",
    "        else:\n",
    "            print(\"\u26a0\ufe0f Possible Issues:\")\n",
    "            print(\"   \u2022 Default learning_rate (0.3) may be too high\")\n",
    "            print(\"   \u2022 May need more trees (n_estimators)\")\n",
    "            print(\"   \u2022 scale_pos_weight alone may not be enough\")\n",
    "    \n",
    "    elif model_name == 'AdaBoost':\n",
    "        if recall < 0.20:\n",
    "            print(\"\u274c Known Limitations:\")\n",
    "            print(\"   \u2022 Uses simple decision stumps (depth=1) by default\")\n",
    "            print(\"   \u2022 Sensitive to noisy data and outliers\")\n",
    "            print(\"   \u2022 No native class_weight support\")\n",
    "            print(\"   \u2022 Often underperforms on imbalanced data\")\n",
    "        else:\n",
    "            print(\"\u26a0\ufe0f Moderate Performance:\")\n",
    "            print(\"   \u2022 Sequential learning helps but limited by weak learners\")\n",
    "            print(\"   \u2022 SAMME algorithm adds some flexibility\")\n",
    "            print(\"   \u2022 May struggle with 4.8:1 imbalance\")\n",
    "    \n",
    "    elif model_name == 'CatBoost':\n",
    "        if recall > 0.25:\n",
    "            print(\"\u2705 Strengths:\")\n",
    "            print(\"   \u2022 Ordered boosting reduces overfitting\")\n",
    "            print(\"   \u2022 Auto class weights handle imbalance\")\n",
    "            print(\"   \u2022 Good default hyperparameters\")\n",
    "            print(\"   \u2022 Handles categorical features well\")\n",
    "        else:\n",
    "            print(\"\u26a0\ufe0f Possible Issues:\")\n",
    "            print(\"   \u2022 May need more iterations\")\n",
    "            print(\"   \u2022 Auto_class_weights may not be aggressive enough\")\n",
    "            print(\"   \u2022 Could benefit from depth tuning\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if there's a clear winner\n",
    "top_2_gap = winner['Recall_Mean'] - runner_up['Recall_Mean']\n",
    "if top_2_gap < 0.01:\n",
    "    print(\"\\n\u2696\ufe0f Top 2 are VERY close:\")\n",
    "    print(f\"   Gap of only {top_2_gap:.3f} means both models are viable\")\n",
    "    print(f\"   Sampling strategies (Phase 3) will be the real differentiator\")\n",
    "else:\n",
    "    print(f\"\\n\ud83c\udfc6 Clear winner: {winner['Model']}\")\n",
    "    print(f\"   {top_2_gap*100:.1f} percentage point lead is significant\")\n",
    "    print(f\"   But both top 2 proceed to Phase 3 for fair comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcbe Step 9: Save Results & Selection for Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SAVING RESULTS FOR PHASE 3\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save comparison dataframe\n",
    "comparison_df.to_csv('model_tournament_results.csv', index=False)\n",
    "print(f\"\\n\u2705 Saved: model_tournament_results.csv\")\n",
    "\n",
    "# Save top 2 models selection\n",
    "top_2_models = top_2['Model'].tolist()\n",
    "selection_data = {\n",
    "    'top_2_models': top_2_models,\n",
    "    'winner': {\n",
    "        'name': winner['Model'],\n",
    "        'recall_mean': float(winner['Recall_Mean']),\n",
    "        'recall_std': float(winner['Recall_Std']),\n",
    "        'precision': float(winner['Precision']),\n",
    "        'f1': float(winner['F1']),\n",
    "        'roc_auc': float(winner['ROC_AUC'])\n",
    "    },\n",
    "    'runner_up': {\n",
    "        'name': runner_up['Model'],\n",
    "        'recall_mean': float(runner_up['Recall_Mean']),\n",
    "        'recall_std': float(runner_up['Recall_Std']),\n",
    "        'precision': float(runner_up['Precision']),\n",
    "        'f1': float(runner_up['F1']),\n",
    "        'roc_auc': float(runner_up['ROC_AUC'])\n",
    "    },\n",
    "    'baseline_recall': baseline_recall,\n",
    "    'winner_improvement': float(winner_improvement)\n",
    "}\n",
    "\n",
    "with open('top_2_models_selection.json', 'w') as f:\n",
    "    json.dump(selection_data, f, indent=2)\n",
    "\n",
    "print(f\"\u2705 Saved: top_2_models_selection.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\ud83c\udfaf PHASE 2 COMPLETE: MODEL TOURNAMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 Selected for Phase 3 (Sampling Strategies):\")\n",
    "print(f\"   1\ufe0f\u20e3 {top_2_models[0]}\")\n",
    "print(f\"   2\ufe0f\u20e3 {top_2_models[1]}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Current Performance:\")\n",
    "print(f\"   Best Recall: {winner['Recall_Mean']:.3f}\")\n",
    "print(f\"   Baseline:    {baseline_recall:.3f}\")\n",
    "print(f\"   Improvement: {winner_improvement:+.3f} ({winner_improvement/baseline_recall*100:+.1f}%)\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf Next: Phase 3 - Sampling Strategy Battle\")\n",
    "print(f\"   Notebook: 05C_sampling_strategies.ipynb\")\n",
    "print(f\"   Will test 3 sampling methods on both selected models\")\n",
    "print(f\"   Goal: Improve recall from {winner['Recall_Mean']:.3f} toward 0.70+\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}