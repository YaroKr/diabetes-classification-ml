{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìã Notebook 02: Data Preparation\n",
    "\n",
    "**Objective:** Prepare two versions of the dataset for modeling\n",
    "\n",
    "**What we'll do:**\n",
    "1. Load the clean data from Notebook 01\n",
    "2. Create Dataset B (Full) - all 21 features\n",
    "3. Create Dataset A (Clean) - remove potentially leaky features\n",
    "4. Prepare scaling strategy\n",
    "5. Save both datasets\n",
    "\n",
    "**Why two datasets?**\n",
    "- Dataset B (Full): Shows maximum predictive power (but might include target leakage)\n",
    "- Dataset A (Clean): More realistic for preventive screening (removes consequences of diabetes)\n",
    "- Comparing them demonstrates critical thinking about feature selection!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For scaling (we'll prepare the strategy, not fit yet)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 2: Load Data from Notebook 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Shape: (253680, 22)\n",
      "\n",
      "Columns: ['Diabetes_012', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('C:\\\\Users\\\\yaros\\\\Desktop\\\\python\\\\faidm\\\\individual_project\\\\diabetes-classification-ml\\\\data\\\\CDC Diabetes Dataset.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA QUALITY VERIFICATION\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ BASIC CHECKS\n",
      "------------------------------------------------------------\n",
      "Total rows: 253,680\n",
      "Total columns: 22\n",
      "Missing values: 0\n",
      "Duplicate rows: 23899\n",
      "\n",
      "2Ô∏è‚É£ DATA TYPES\n",
      "------------------------------------------------------------\n",
      "float64    22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚ö†Ô∏è All columns should be numeric (float64 or int64)\n",
      "\n",
      "3Ô∏è‚É£ NON-NUMERIC VALUES CHECK\n",
      "------------------------------------------------------------\n",
      "‚úÖ All columns are numeric\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive data quality check\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Basic checks\n",
    "print(\"\\n1Ô∏è‚É£ BASIC CHECKS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# 2. Data types check\n",
    "print(\"\\n2Ô∏è‚É£ DATA TYPES\")\n",
    "print(\"-\" * 60)\n",
    "print(df.dtypes.value_counts())\n",
    "print(f\"\\n‚ö†Ô∏è All columns should be numeric (float64 or int64)\")\n",
    "\n",
    "# 3. Check for any non-numeric values\n",
    "print(\"\\n3Ô∏è‚É£ NON-NUMERIC VALUES CHECK\")\n",
    "print(\"-\" * 60)\n",
    "non_numeric_cols = df.select_dtypes(exclude=['number']).columns.tolist()\n",
    "if non_numeric_cols:\n",
    "    print(f\"‚ö†Ô∏è Non-numeric columns found: {non_numeric_cols}\")\n",
    "else:\n",
    "    print(\"‚úÖ All columns are numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4Ô∏è‚É£ VALUE RANGE VERIFICATION\n",
      "------------------------------------------------------------\n",
      "Checking if values are within expected ranges...\n",
      "\n",
      "‚úÖ All features are within expected ranges\n"
     ]
    }
   ],
   "source": [
    "# 4. Check value ranges for each feature\n",
    "print(\"\\n4Ô∏è‚É£ VALUE RANGE VERIFICATION\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Checking if values are within expected ranges...\\n\")\n",
    "\n",
    "# Expected ranges based on dataset description\n",
    "expected_ranges = {\n",
    "    'Diabetes_012': (0, 2),\n",
    "    'HighBP': (0, 1),\n",
    "    'HighChol': (0, 1),\n",
    "    'CholCheck': (0, 1),\n",
    "    'BMI': (12, 98),  # Reasonable human BMI range\n",
    "    'Smoker': (0, 1),\n",
    "    'Stroke': (0, 1),\n",
    "    'HeartDiseaseorAttack': (0, 1),\n",
    "    'PhysActivity': (0, 1),\n",
    "    'Fruits': (0, 1),\n",
    "    'Veggies': (0, 1),\n",
    "    'HvyAlcoholConsump': (0, 1),\n",
    "    'AnyHealthcare': (0, 1),\n",
    "    'NoDocbcCost': (0, 1),\n",
    "    'GenHlth': (1, 5),\n",
    "    'MentHlth': (0, 30),\n",
    "    'PhysHlth': (0, 30),\n",
    "    'DiffWalk': (0, 1),\n",
    "    'Sex': (0, 1),\n",
    "    'Age': (1, 13),\n",
    "    'Education': (1, 6),\n",
    "    'Income': (1, 8)\n",
    "}\n",
    "\n",
    "range_issues = []\n",
    "\n",
    "for col, (min_val, max_val) in expected_ranges.items():\n",
    "    actual_min = df[col].min()\n",
    "    actual_max = df[col].max()\n",
    "    \n",
    "    if actual_min < min_val or actual_max > max_val:\n",
    "        range_issues.append(col)\n",
    "        print(f\"‚ö†Ô∏è {col}: Expected [{min_val}-{max_val}], Got [{actual_min}-{actual_max}]\")\n",
    "\n",
    "if not range_issues:\n",
    "    print(\"‚úÖ All features are within expected ranges\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Found {len(range_issues)} features with unexpected ranges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5Ô∏è‚É£ OUTLIER DETECTION (Continuous Features)\n",
      "------------------------------------------------------------\n",
      "\n",
      "BMI:\n",
      "  Range: [12.0 - 98.0]\n",
      "  Mean: 28.4, Median: 27.0\n",
      "  IQR bounds: [13.5 - 41.5]\n",
      "  Outliers: 9,847 (3.88%)\n",
      "  ‚úÖ Outlier percentage acceptable\n",
      "\n",
      "MentHlth:\n",
      "  Range: [0.0 - 30.0]\n",
      "  Mean: 3.2, Median: 0.0\n",
      "  IQR bounds: [-3.0 - 5.0]\n",
      "  Outliers: 36,208 (14.27%)\n",
      "  ‚ö†Ô∏è High percentage of outliers (>5%)\n",
      "\n",
      "PhysHlth:\n",
      "  Range: [0.0 - 30.0]\n",
      "  Mean: 4.2, Median: 0.0\n",
      "  IQR bounds: [-4.5 - 7.5]\n",
      "  Outliers: 40,949 (16.14%)\n",
      "  ‚ö†Ô∏è High percentage of outliers (>5%)\n"
     ]
    }
   ],
   "source": [
    "# 5. Check for outliers in continuous features\n",
    "print(\"\\n5Ô∏è‚É£ OUTLIER DETECTION (Continuous Features)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "continuous_cols = ['BMI', 'MentHlth', 'PhysHlth']\n",
    "\n",
    "for col in continuous_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    outlier_pct = (len(outliers) / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Range: [{df[col].min():.1f} - {df[col].max():.1f}]\")\n",
    "    print(f\"  Mean: {df[col].mean():.1f}, Median: {df[col].median():.1f}\")\n",
    "    print(f\"  IQR bounds: [{lower_bound:.1f} - {upper_bound:.1f}]\")\n",
    "    print(f\"  Outliers: {len(outliers):,} ({outlier_pct:.2f}%)\")\n",
    "    \n",
    "    if outlier_pct > 5:\n",
    "        print(f\"  ‚ö†Ô∏è High percentage of outliers (>5%)\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ Outlier percentage acceptable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6Ô∏è‚É£ BINARY FEATURE DISTRIBUTION CHECK\n",
      "------------------------------------------------------------\n",
      "\n",
      "Checking if binary features only contain 0 and 1...\n",
      "\n",
      "‚úÖ All binary features contain only 0 and 1\n"
     ]
    }
   ],
   "source": [
    "# 6. Check for unexpected value distributions in binary features\n",
    "print(\"\\n6Ô∏è‚É£ BINARY FEATURE DISTRIBUTION CHECK\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "binary_cols = ['HighBP', 'HighChol', 'CholCheck', 'Smoker', 'Stroke', \n",
    "               'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies',\n",
    "               'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', \n",
    "               'DiffWalk', 'Sex']\n",
    "\n",
    "print(\"\\nChecking if binary features only contain 0 and 1...\\n\")\n",
    "\n",
    "binary_issues = []\n",
    "for col in binary_cols:\n",
    "    unique_vals = df[col].unique()\n",
    "    if not set(unique_vals).issubset({0.0, 1.0}):\n",
    "        binary_issues.append(col)\n",
    "        print(f\"‚ö†Ô∏è {col}: Contains values other than 0/1: {unique_vals}\")\n",
    "\n",
    "if not binary_issues:\n",
    "    print(\"‚úÖ All binary features contain only 0 and 1\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Found {len(binary_issues)} binary features with unexpected values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7Ô∏è‚É£ TARGET VARIABLE CHECK\n",
      "------------------------------------------------------------\n",
      "\n",
      "Class distribution:\n",
      "  Class 0: 213,703 (84.24%)\n",
      "  Class 1:  4,631 ( 1.83%)\n",
      "  Class 2: 35,346 (13.93%)\n",
      "\n",
      "Imbalance ratio: 46.1:1\n",
      "‚ö†Ô∏è SEVERE class imbalance detected (>10:1)\n",
      "   ‚Üí We'll need to handle this in modeling phase\n"
     ]
    }
   ],
   "source": [
    "# 7. Target variable distribution check\n",
    "print(\"\\n7Ô∏è‚É£ TARGET VARIABLE CHECK\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "target_col = 'Diabetes_012'\n",
    "target_counts = df[target_col].value_counts().sort_index()\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "for cls, count in target_counts.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"  Class {int(cls)}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "# Calculate imbalance ratio\n",
    "majority_class = target_counts.max()\n",
    "minority_class = target_counts.min()\n",
    "imbalance_ratio = majority_class / minority_class\n",
    "\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.1f}:1\")\n",
    "if imbalance_ratio > 10:\n",
    "    print(\"‚ö†Ô∏è SEVERE class imbalance detected (>10:1)\")\n",
    "    print(\"   ‚Üí We'll need to handle this in modeling phase\")\n",
    "elif imbalance_ratio > 3:\n",
    "    print(\"‚ö†Ô∏è Moderate class imbalance detected (>3:1)\")\n",
    "else:\n",
    "    print(\"‚úÖ Classes are relatively balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL DATA QUALITY SUMMARY\n",
      "============================================================\n",
      "\n",
      "‚úÖ No missing values\n",
      "‚ö†Ô∏è No duplicates\n",
      "‚úÖ All numeric types\n",
      "‚úÖ Values in expected ranges\n",
      "‚úÖ Binary features valid\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è SOME ISSUES DETECTED - Review above for details\n",
      "   (Note: Some issues like outliers may be expected)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 8. Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "quality_checks = {\n",
    "    'No missing values': df.isnull().sum().sum() == 0,\n",
    "    'No duplicates': df.duplicated().sum() == 0,\n",
    "    'All numeric types': len(non_numeric_cols) == 0,\n",
    "    'Values in expected ranges': len(range_issues) == 0,\n",
    "    'Binary features valid': len(binary_issues) == 0,\n",
    "}\n",
    "\n",
    "print()\n",
    "for check, passed in quality_checks.items():\n",
    "    status = \"‚úÖ\" if passed else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} {check}\")\n",
    "\n",
    "all_passed = all(quality_checks.values())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_passed:\n",
    "    print(\"‚úÖ ALL DATA QUALITY CHECKS PASSED!\")\n",
    "    print(\"‚úÖ Dataset is ready for preprocessing and modeling\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SOME ISSUES DETECTED - Review above for details\")\n",
    "    print(\"   (Note: Some issues like outliers may be expected)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 3: Remove Duplicates FIRST\n",
    "\n",
    "**Strategy:** Remove duplicates from the original dataset BEFORE creating Dataset A and Dataset B.\n",
    "\n",
    "**Why this order?**\n",
    "- Ensures both datasets have the same sample size\n",
    "- Makes model comparison fair (only difference is features, not samples)\n",
    "- Cleaner methodology\n",
    "\n",
    "**Important:** After this step, we work with deduplicated data for ALL subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: REMOVE DUPLICATES FROM ORIGINAL DATASET\n",
      "============================================================\n",
      "\n",
      "üìä BEFORE deduplication:\n",
      "Total rows: 253,680\n",
      "Duplicate rows: 23,899 (9.42%)\n",
      "Unique rows: 229,781\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: REMOVE DUPLICATES FROM ORIGINAL DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count duplicates in original data\n",
    "duplicates_original = df.duplicated().sum()\n",
    "duplicate_pct = (duplicates_original / len(df)) * 100\n",
    "\n",
    "print(f\"\\nüìä BEFORE deduplication:\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Duplicate rows: {duplicates_original:,} ({duplicate_pct:.2f}%)\")\n",
    "print(f\"Unique rows: {len(df) - duplicates_original:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ AFTER deduplication:\n",
      "Before: 253,680 rows\n",
      "After:  229,781 rows\n",
      "Removed: 23,899 duplicate rows\n",
      "\n",
      "‚úÖ Verification: 0 duplicates remaining (should be 0)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates and store in NEW variable\n",
    "df_deduplicated = df.drop_duplicates().copy()\n",
    "\n",
    "print(f\"\\n‚úÖ AFTER deduplication:\")\n",
    "print(f\"Before: {len(df):,} rows\")\n",
    "print(f\"After:  {len(df_deduplicated):,} rows\")\n",
    "print(f\"Removed: {len(df) - len(df_deduplicated):,} duplicate rows\")\n",
    "\n",
    "# Verify no duplicates remain\n",
    "remaining_duplicates = df_deduplicated.duplicated().sum()\n",
    "print(f\"\\n‚úÖ Verification: {remaining_duplicates} duplicates remaining (should be 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Class distribution AFTER deduplication:\n",
      "------------------------------------------------------------\n",
      "Class 0: 190,055 (82.71%) - Original: 84.24%\n",
      "Class 1:  4,629 ( 2.01%) - Original:  1.83%\n",
      "Class 2: 35,097 (15.27%) - Original: 13.93%\n",
      "\n",
      "‚úÖ Class proportions preserved after deduplication!\n"
     ]
    }
   ],
   "source": [
    "# Check class balance is preserved\n",
    "print(f\"\\nüìä Class distribution AFTER deduplication:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for cls, count in df_deduplicated['Diabetes_012'].value_counts().sort_index().items():\n",
    "    pct = (count / len(df_deduplicated)) * 100\n",
    "    original_pct = (df['Diabetes_012'].value_counts().sort_index()[cls] / len(df)) * 100\n",
    "    print(f\"Class {int(cls)}: {count:6,} ({pct:5.2f}%) - Original: {original_pct:5.2f}%\")\n",
    "\n",
    "print(\"\\n‚úÖ Class proportions preserved after deduplication!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ DEDUPLICATED DATASET READY\n",
      "============================================================\n",
      "\n",
      "Working dataset: 229,781 rows, 22 columns\n",
      "\n",
      "‚ö†Ô∏è IMPORTANT: All subsequent steps will use df_deduplicated\n"
     ]
    }
   ],
   "source": [
    "# FROM NOW ON, use df_deduplicated for ALL subsequent work\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ DEDUPLICATED DATASET READY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nWorking dataset: {len(df_deduplicated):,} rows, {len(df_deduplicated.columns)} columns\")\n",
    "print(f\"\\n‚ö†Ô∏è IMPORTANT: All subsequent steps will use df_deduplicated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 4: Identify Potentially Leaky Features\n",
    "\n",
    "**Target Leakage** occurs when a feature is a *consequence* of the target variable, rather than a *cause*.\n",
    "\n",
    "**Why this matters:**\n",
    "- If we include leaky features, our model might look great in testing...\n",
    "- But it won't work for **preventive screening** (before diabetes develops)\n",
    "- It would only work for **diagnostic confirmation** (after symptoms appear)\n",
    "\n",
    "**Potentially leaky features in this dataset:**\n",
    "\n",
    "| Feature | Description | Why It Might Be Leaky |\n",
    "|---------|-------------|----------------------|\n",
    "| `DiffWalk` | Difficulty walking or climbing stairs | Often a **consequence** of diabetes (neuropathy, poor circulation) |\n",
    "| `GenHlth` | Self-reported general health (1-5 scale) | People with diabetes naturally rate their health lower |\n",
    "| `PhysHlth` | Days of poor physical health (0-30) | Similar to GenHlth - likely consequence of diabetes |\n",
    "\n",
    "**Our strategy:**\n",
    "1. Create **Dataset B (Full)** - keep all features (shows maximum predictive power)\n",
    "2. Create **Dataset A (Clean)** - remove these 3 features (more realistic for prevention)\n",
    "3. Compare model performance on both\n",
    "4. Discuss implications in final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potentially leaky features identified:\n",
      "  - DiffWalk\n",
      "  - GenHlth\n",
      "  - PhysHlth\n",
      "\n",
      "These will be removed in Dataset A (Clean)\n"
     ]
    }
   ],
   "source": [
    "# Define features to remove for clean dataset\n",
    "potentially_leaky_features = ['DiffWalk', 'GenHlth', 'PhysHlth']\n",
    "\n",
    "print(\"Potentially leaky features identified:\")\n",
    "for feature in potentially_leaky_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "print(f\"\\nThese will be removed in Dataset A (Clean)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Step 4: Identify Potentially Leaky Features\n",
    "\n",
    "**Target Leakage** occurs when a feature is a *consequence* of the target variable, rather than a *cause*.\n",
    "\n",
    "**Why this matters:**\n",
    "- If we include leaky features, our model might look great in testing...\n",
    "- But it won't work for **preventive screening** (before diabetes develops)\n",
    "- It would only work for **diagnostic confirmation** (after symptoms appear)\n",
    "\n",
    "**Potentially leaky features in this dataset:**\n",
    "\n",
    "| Feature | Description | Why It Might Be Leaky |\n",
    "|---------|-------------|----------------------|\n",
    "| `DiffWalk` | Difficulty walking or climbing stairs | Often a **consequence** of diabetes (neuropathy, poor circulation) |\n",
    "| `GenHlth` | Self-reported general health (1-5 scale) | People with diabetes naturally rate their health lower |\n",
    "| `PhysHlth` | Days of poor physical health (0-30) | Similar to GenHlth - likely consequence of diabetes |\n",
    "\n",
    "**Our strategy:**\n",
    "1. Create **Dataset B (Full)** - keep all features (shows maximum predictive power)\n",
    "2. Create **Dataset A (Clean)** - remove these 3 features (more realistic for prevention)\n",
    "3. Compare model performance on both\n",
    "4. Discuss implications in final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potentially leaky features identified:\n",
      "  - DiffWalk\n",
      "  - GenHlth\n",
      "  - PhysHlth\n",
      "\n",
      "These will be removed in Dataset A (Clean)\n"
     ]
    }
   ],
   "source": [
    "# Define features to remove for clean dataset\n",
    "potentially_leaky_features = ['DiffWalk', 'GenHlth', 'PhysHlth']\n",
    "\n",
    "print(\"Potentially leaky features identified:\")\n",
    "for feature in potentially_leaky_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "print(f\"\\nThese will be removed in Dataset A (Clean)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 5: Create Dataset B (Full) - All Features\n",
    "\n",
    "**Dataset B includes all 21 features from the DEDUPLICATED data.**\n",
    "\n",
    "This represents the \"best case scenario\" where we have access to all available information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 5: CREATING DATASET B (FULL) - ALL FEATURES\n",
      "============================================================\n",
      "\n",
      "‚úÖ Created Dataset B from deduplicated data\n",
      "Source: df_deduplicated (229,781 rows)\n",
      "Result: df_full (229,781 rows)\n",
      "\n",
      "‚úÖ Verification passed: Same size as deduplicated data\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 5: CREATING DATASET B (FULL) - ALL FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CRITICAL: Use df_deduplicated (not df!)\n",
    "df_full = df_deduplicated.copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Created Dataset B from deduplicated data\")\n",
    "print(f\"Source: df_deduplicated ({len(df_deduplicated):,} rows)\")\n",
    "print(f\"Result: df_full ({len(df_full):,} rows)\")\n",
    "\n",
    "# Verify\n",
    "assert len(df_full) == len(df_deduplicated), \"‚ùå ERROR: df_full has different size!\"\n",
    "print(f\"\\n‚úÖ Verification passed: Same size as deduplicated data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset B (Full) ===\n",
      "Samples: 229,781\n",
      "Features shape: (229781, 21)\n",
      "Target shape: (229781,)\n",
      "\n",
      "Number of features: 21\n",
      "\n",
      "Feature list:\n",
      "   1. HighBP\n",
      "   2. HighChol\n",
      "   3. CholCheck\n",
      "   4. BMI\n",
      "   5. Smoker\n",
      "   6. Stroke\n",
      "   7. HeartDiseaseorAttack\n",
      "   8. PhysActivity\n",
      "   9. Fruits\n",
      "  10. Veggies\n",
      "  11. HvyAlcoholConsump\n",
      "  12. AnyHealthcare\n",
      "  13. NoDocbcCost\n",
      "  14. GenHlth\n",
      "  15. MentHlth\n",
      "  16. PhysHlth\n",
      "  17. DiffWalk\n",
      "  18. Sex\n",
      "  19. Age\n",
      "  20. Education\n",
      "  21. Income\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target for Dataset B\n",
    "X_full = df_full.drop('Diabetes_012', axis=1)\n",
    "y_full = df_full['Diabetes_012']\n",
    "\n",
    "print(\"=== Dataset B (Full) ===\")\n",
    "print(f\"Samples: {len(df_full):,}\")\n",
    "print(f\"Features shape: {X_full.shape}\")\n",
    "print(f\"Target shape: {y_full.shape}\")\n",
    "print(f\"\\nNumber of features: {X_full.shape[1]}\")\n",
    "print(f\"\\nFeature list:\")\n",
    "for i, col in enumerate(X_full.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Step 6: Create Dataset A (Clean) - Remove Leaky Features\n",
    "\n",
    "**Dataset A removes potentially leaky features from the DEDUPLICATED data.**\n",
    "\n",
    "This represents a more realistic scenario for preventive screening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 6: CREATING DATASET A (CLEAN) - REMOVE LEAKY FEATURES\n",
      "============================================================\n",
      "\n",
      "üö´ Removing features: ['DiffWalk', 'GenHlth', 'PhysHlth']\n",
      "\n",
      "‚úÖ Created Dataset A from deduplicated data\n",
      "Source: df_deduplicated (229,781 rows)\n",
      "Result: df_clean (229,781 rows)\n",
      "\n",
      "‚úÖ Verification passed: Same size as deduplicated data\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 6: CREATING DATASET A (CLEAN) - REMOVE LEAKY FEATURES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüö´ Removing features: {potentially_leaky_features}\")\n",
    "\n",
    "# CRITICAL: Use df_deduplicated (not df!)\n",
    "df_clean = df_deduplicated.drop(columns=potentially_leaky_features).copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Created Dataset A from deduplicated data\")\n",
    "print(f\"Source: df_deduplicated ({len(df_deduplicated):,} rows)\")\n",
    "print(f\"Result: df_clean ({len(df_clean):,} rows)\")\n",
    "\n",
    "# Verify\n",
    "assert len(df_clean) == len(df_deduplicated), \"‚ùå ERROR: df_clean has different size!\"\n",
    "print(f\"\\n‚úÖ Verification passed: Same size as deduplicated data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset A (Clean) ===\n",
      "Samples: 229,781\n",
      "Features shape: (229781, 18)\n",
      "Target shape: (229781,)\n",
      "\n",
      "Number of features: 18\n",
      "\n",
      "Removed features: ['DiffWalk', 'GenHlth', 'PhysHlth']\n",
      "\n",
      "Remaining feature list:\n",
      "   1. HighBP\n",
      "   2. HighChol\n",
      "   3. CholCheck\n",
      "   4. BMI\n",
      "   5. Smoker\n",
      "   6. Stroke\n",
      "   7. HeartDiseaseorAttack\n",
      "   8. PhysActivity\n",
      "   9. Fruits\n",
      "  10. Veggies\n",
      "  11. HvyAlcoholConsump\n",
      "  12. AnyHealthcare\n",
      "  13. NoDocbcCost\n",
      "  14. MentHlth\n",
      "  15. Sex\n",
      "  16. Age\n",
      "  17. Education\n",
      "  18. Income\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target for Dataset A\n",
    "X_clean = df_clean.drop('Diabetes_012', axis=1)\n",
    "y_clean = df_clean['Diabetes_012']\n",
    "\n",
    "print(\"=== Dataset A (Clean) ===\")\n",
    "print(f\"Samples: {len(df_clean):,}\")\n",
    "print(f\"Features shape: {X_clean.shape}\")\n",
    "print(f\"Target shape: {y_clean.shape}\")\n",
    "print(f\"\\nNumber of features: {X_clean.shape[1]}\")\n",
    "print(f\"\\nRemoved features: {potentially_leaky_features}\")\n",
    "print(f\"\\nRemaining feature list:\")\n",
    "for i, col in enumerate(X_clean.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 7: Verify Both Datasets Are Identical in Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFICATION: BOTH DATASETS SAME SIZE\n",
      "============================================================\n",
      "\n",
      "üìä Dataset Comparison:\n",
      "Dataset B (Full):  229,781 samples, 21 features\n",
      "Dataset A (Clean): 229,781 samples, 18 features\n",
      "\n",
      "‚úÖ Same sample size? True\n",
      "‚úÖ Same target values? True\n",
      "\n",
      "üìä Target distribution (both datasets):\n",
      "------------------------------------------------------------\n",
      "Class 0: 190,055 (82.71%)\n",
      "Class 1:  4,629 ( 2.01%)\n",
      "Class 2: 35,097 (15.27%)\n",
      "\n",
      "============================================================\n",
      "‚úÖ BOTH DATASETS READY FOR MODELING\n",
      "============================================================\n",
      "\n",
      "Dataset B: 229,781 samples, 21 features (all features)\n",
      "Dataset A: 229,781 samples, 18 features (removed 3 leaky features)\n",
      "\n",
      "üí° Same sample size ensures fair model comparison!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION: BOTH DATASETS SAME SIZE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Dataset Comparison:\")\n",
    "print(f\"Dataset B (Full):  {len(df_full):,} samples, {X_full.shape[1]} features\")\n",
    "print(f\"Dataset A (Clean): {len(df_clean):,} samples, {X_clean.shape[1]} features\")\n",
    "\n",
    "# Critical checks\n",
    "same_size = len(df_full) == len(df_clean)\n",
    "same_target = y_full.equals(y_clean)\n",
    "\n",
    "print(f\"\\n‚úÖ Same sample size? {same_size}\")\n",
    "if not same_size:\n",
    "    print(f\"   ‚ùå ERROR: Dataset sizes don't match!\")\n",
    "    print(f\"   Dataset B: {len(df_full):,}\")\n",
    "    print(f\"   Dataset A: {len(df_clean):,}\")\n",
    "    print(f\"   Difference: {abs(len(df_full) - len(df_clean)):,}\")\n",
    "    raise AssertionError(\"Dataset sizes must be identical!\")\n",
    "\n",
    "print(f\"‚úÖ Same target values? {same_target}\")\n",
    "if not same_target:\n",
    "    print(f\"   ‚ùå ERROR: Targets don't match!\")\n",
    "    raise AssertionError(\"Targets must be identical!\")\n",
    "\n",
    "print(f\"\\nüìä Target distribution (both datasets):\")\n",
    "print(\"-\" * 60)\n",
    "for cls, count in y_full.value_counts().sort_index().items():\n",
    "    pct = (count / len(y_full)) * 100\n",
    "    print(f\"Class {int(cls)}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ BOTH DATASETS READY FOR MODELING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset B: {len(df_full):,} samples, {X_full.shape[1]} features (all features)\")\n",
    "print(f\"Dataset A: {len(df_clean):,} samples, {X_clean.shape[1]} features (removed 3 leaky features)\")\n",
    "print(f\"\\nüí° Same sample size ensures fair model comparison!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Step 8: Feature Scaling Strategy\n",
    "\n",
    "**Why do we need scaling?**\n",
    "\n",
    "Different features have different ranges:\n",
    "- `BMI`: ranges from 14 to 98\n",
    "- `Age`: ranges from 1 to 13\n",
    "- Binary features: only 0 or 1\n",
    "\n",
    "**Which algorithms need scaling?**\n",
    "- ‚úÖ **Need scaling:** Logistic Regression, SVM, KNN (distance-based)\n",
    "- ‚ùå **Don't need scaling:** Random Forest, Decision Trees, XGBoost (tree-based)\n",
    "\n",
    "**Our approach:**\n",
    "- We'll use `StandardScaler` (mean=0, std=1)\n",
    "- Apply it ONLY to continuous features: `BMI`, `MentHlth`, `PhysHlth` (if present)\n",
    "- Leave binary/ordinal features as-is\n",
    "\n",
    "**IMPORTANT:** We'll fit the scaler later (in training pipeline) to avoid data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Types for Scaling ===\n",
      "\n",
      "Dataset B (Full) - Continuous features to scale:\n",
      "  - BMI: range [12 - 98]\n",
      "  - MentHlth: range [0 - 30]\n",
      "  - PhysHlth: range [0 - 30]\n",
      "\n",
      "Dataset A (Clean) - Continuous features to scale:\n",
      "  - BMI: range [12 - 98]\n",
      "  - MentHlth: range [0 - 30]\n",
      "\n",
      "‚úÖ We'll apply StandardScaler to these features in the modeling pipeline\n"
     ]
    }
   ],
   "source": [
    "# Identify feature types for scaling\n",
    "print(\"=== Feature Types for Scaling ===\")\n",
    "\n",
    "# Continuous features that need scaling\n",
    "continuous_features_full = ['BMI', 'MentHlth', 'PhysHlth']  # For Dataset B\n",
    "continuous_features_clean = ['BMI', 'MentHlth']             # For Dataset A (PhysHlth removed)\n",
    "\n",
    "print(f\"\\nDataset B (Full) - Continuous features to scale:\")\n",
    "for feat in continuous_features_full:\n",
    "    if feat in X_full.columns:\n",
    "        print(f\"  - {feat}: range [{X_full[feat].min():.0f} - {X_full[feat].max():.0f}]\")\n",
    "\n",
    "print(f\"\\nDataset A (Clean) - Continuous features to scale:\")\n",
    "for feat in continuous_features_clean:\n",
    "    if feat in X_clean.columns:\n",
    "        print(f\"  - {feat}: range [{X_clean[feat].min():.0f} - {X_clean[feat].max():.0f}]\")\n",
    "\n",
    "print(f\"\\n‚úÖ We'll apply StandardScaler to these features in the modeling pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Step 8: Feature Scaling Strategy\n",
    "\n",
    "**Why do we need scaling?**\n",
    "\n",
    "Different features have different ranges:\n",
    "- `BMI`: ranges from 14 to 98\n",
    "- `Age`: ranges from 1 to 13\n",
    "- Binary features: only 0 or 1\n",
    "\n",
    "**Which algorithms need scaling?**\n",
    "- ‚úÖ **Need scaling:** Logistic Regression, SVM, KNN (distance-based)\n",
    "- ‚ùå **Don't need scaling:** Random Forest, Decision Trees, XGBoost (tree-based)\n",
    "\n",
    "**Our approach:**\n",
    "- We'll use `StandardScaler` (mean=0, std=1)\n",
    "- Apply it ONLY to continuous features: `BMI`, `MentHlth`, `PhysHlth` (if present)\n",
    "- Leave binary/ordinal features as-is\n",
    "\n",
    "**IMPORTANT:** We'll fit the scaler later (in training pipeline) to avoid data leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Types for Scaling ===\n",
      "\n",
      "Dataset B (Full) - Continuous features to scale:\n",
      "  - BMI: range [12 - 98]\n",
      "  - MentHlth: range [0 - 30]\n",
      "  - PhysHlth: range [0 - 30]\n",
      "\n",
      "Dataset A (Clean) - Continuous features to scale:\n",
      "  - BMI: range [12 - 98]\n",
      "  - MentHlth: range [0 - 30]\n",
      "\n",
      "‚úÖ We'll apply StandardScaler to these features in the modeling pipeline\n"
     ]
    }
   ],
   "source": [
    "# Identify feature types for scaling\n",
    "print(\"=== Feature Types for Scaling ===\")\n",
    "\n",
    "# Continuous features that need scaling\n",
    "continuous_features_full = ['BMI', 'MentHlth', 'PhysHlth']  # For Dataset B\n",
    "continuous_features_clean = ['BMI', 'MentHlth']             # For Dataset A (PhysHlth removed)\n",
    "\n",
    "print(f\"\\nDataset B (Full) - Continuous features to scale:\")\n",
    "for feat in continuous_features_full:\n",
    "    if feat in X_full.columns:\n",
    "        print(f\"  - {feat}: range [{X_full[feat].min():.0f} - {X_full[feat].max():.0f}]\")\n",
    "\n",
    "print(f\"\\nDataset A (Clean) - Continuous features to scale:\")\n",
    "for feat in continuous_features_clean:\n",
    "    if feat in X_clean.columns:\n",
    "        print(f\"  - {feat}: range [{X_clean[feat].min():.0f} - {X_clean[feat].max():.0f}]\")\n",
    "\n",
    "print(f\"\\n‚úÖ We'll apply StandardScaler to these features in the modeling pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 9: Save Prepared Datasets\n",
    "\n",
    "We'll save both datasets for use in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: dataset_B_full.csv\n",
      "   Shape: (229781, 22)\n",
      "   Features: 21 (+ 1 target)\n",
      "\n",
      "‚úÖ Saved: dataset_A_clean.csv\n",
      "   Shape: (229781, 19)\n",
      "   Features: 18 (+ 1 target)\n",
      "\n",
      "============================================================\n",
      "‚úÖ Data preparation complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Save Dataset B (Full)\n",
    "df_full.to_csv('dataset_B_full.csv', index=False)\n",
    "print(\"‚úÖ Saved: dataset_B_full.csv\")\n",
    "print(f\"   Shape: {df_full.shape}\")\n",
    "print(f\"   Features: {df_full.shape[1] - 1} (+ 1 target)\")\n",
    "\n",
    "# Save Dataset A (Clean)\n",
    "df_clean.to_csv('dataset_A_clean.csv', index=False)\n",
    "print(\"\\n‚úÖ Saved: dataset_A_clean.csv\")\n",
    "print(f\"   Shape: {df_clean.shape}\")\n",
    "print(f\"   Features: {df_clean.shape[1] - 1} (+ 1 target)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Data preparation complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 8: Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset Comparison ===\n",
      "          Dataset  Total Samples  Features            Removed Features                       Use Case\n",
      " Dataset B (Full)         229781        21                           -       Maximum predictive power\n",
      "Dataset A (Clean)         229781        18 DiffWalk, GenHlth, PhysHlth Realistic preventive screening\n"
     ]
    }
   ],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Dataset': ['Dataset B (Full)', 'Dataset A (Clean)'],\n",
    "    'Total Samples': [len(df_full), len(df_clean)],\n",
    "    'Features': [X_full.shape[1], X_clean.shape[1]],\n",
    "    'Removed Features': ['-', ', '.join(potentially_leaky_features)],\n",
    "    'Use Case': ['Maximum predictive power', 'Realistic preventive screening']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n=== Dataset Comparison ===\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING AND VERIFYING SAVED DATASETS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Both datasets loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "## üîç VERIFICATION: Load and Inspect Both Datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING AND VERIFYING SAVED DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load both datasets\n",
    "df_full = pd.read_csv('C:\\\\Users\\\\yaros\\\\Desktop\\\\python\\\\faidm\\\\individual_project\\\\diabetes-classification-ml\\\\data_preparation\\\\dataset_B_full.csv')\n",
    "df_clean = pd.read_csv('C:\\\\Users\\\\yaros\\\\Desktop\\\\python\\\\faidm\\\\individual_project\\\\diabetes-classification-ml\\\\data_preparation\\\\dataset_A_clean.csv')\n",
    "\n",
    "print(\"\\n‚úÖ Both datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "1Ô∏è‚É£ SHAPE COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Dataset B (Full):\n",
      "  Rows: 229,781\n",
      "  Columns: 22\n",
      "  Total cells: 5,055,182\n",
      "\n",
      "Dataset A (Clean):\n",
      "  Rows: 229,781\n",
      "  Columns: 19\n",
      "  Total cells: 4,365,839\n",
      "\n",
      "üìä Comparison:\n",
      "  Same number of rows? True ‚úÖ\n",
      "  Column difference: 3 columns\n"
     ]
    }
   ],
   "source": [
    "# 1. Basic shape comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"1Ô∏è‚É£ SHAPE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nDataset B (Full):\")\n",
    "print(f\"  Rows: {len(df_full):,}\")\n",
    "print(f\"  Columns: {len(df_full.columns)}\")\n",
    "print(f\"  Total cells: {df_full.size:,}\")\n",
    "\n",
    "print(f\"\\nDataset A (Clean):\")\n",
    "print(f\"  Rows: {len(df_clean):,}\")\n",
    "print(f\"  Columns: {len(df_clean.columns)}\")\n",
    "print(f\"  Total cells: {df_clean.size:,}\")\n",
    "\n",
    "print(f\"\\nüìä Comparison:\")\n",
    "print(f\"  Same number of rows? {len(df_full) == len(df_clean)} ‚úÖ\")\n",
    "print(f\"  Column difference: {len(df_full.columns) - len(df_clean.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "2Ô∏è‚É£ COLUMN COMPARISON\n",
      "======================================================================\n",
      "\n",
      "üìã Dataset B (Full) - 22 columns:\n",
      "----------------------------------------------------------------------\n",
      "   1. Diabetes_012\n",
      "   2. HighBP\n",
      "   3. HighChol\n",
      "   4. CholCheck\n",
      "   5. BMI\n",
      "   6. Smoker\n",
      "   7. Stroke\n",
      "   8. HeartDiseaseorAttack\n",
      "   9. PhysActivity\n",
      "  10. Fruits\n",
      "  11. Veggies\n",
      "  12. HvyAlcoholConsump\n",
      "  13. AnyHealthcare\n",
      "  14. NoDocbcCost\n",
      "  15. GenHlth\n",
      "  16. MentHlth\n",
      "  17. PhysHlth\n",
      "  18. DiffWalk\n",
      "  19. Sex\n",
      "  20. Age\n",
      "  21. Education\n",
      "  22. Income\n",
      "\n",
      "üìã Dataset A (Clean) - 19 columns:\n",
      "----------------------------------------------------------------------\n",
      "   1. Diabetes_012\n",
      "   2. HighBP\n",
      "   3. HighChol\n",
      "   4. CholCheck\n",
      "   5. BMI\n",
      "   6. Smoker\n",
      "   7. Stroke\n",
      "   8. HeartDiseaseorAttack\n",
      "   9. PhysActivity\n",
      "  10. Fruits\n",
      "  11. Veggies\n",
      "  12. HvyAlcoholConsump\n",
      "  13. AnyHealthcare\n",
      "  14. NoDocbcCost\n",
      "  15. MentHlth\n",
      "  16. Sex\n",
      "  17. Age\n",
      "  18. Education\n",
      "  19. Income\n",
      "\n",
      "üö´ Columns ONLY in Dataset B (removed in Dataset A):\n",
      "----------------------------------------------------------------------\n",
      "  - DiffWalk\n",
      "  - GenHlth\n",
      "  - PhysHlth\n",
      "\n",
      "‚ûï Columns ONLY in Dataset A (not in Dataset B):\n",
      "----------------------------------------------------------------------\n",
      "  None (expected)\n"
     ]
    }
   ],
   "source": [
    "# 2. Column comparison - What's in each dataset?\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2Ô∏è‚É£ COLUMN COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìã Dataset B (Full) - {len(df_full.columns)} columns:\")\n",
    "print(\"-\" * 70)\n",
    "for i, col in enumerate(df_full.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nüìã Dataset A (Clean) - {len(df_clean.columns)} columns:\")\n",
    "print(\"-\" * 70)\n",
    "for i, col in enumerate(df_clean.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Find which columns are in B but not in A\n",
    "columns_in_B_not_A = set(df_full.columns) - set(df_clean.columns)\n",
    "columns_in_A_not_B = set(df_clean.columns) - set(df_full.columns)\n",
    "\n",
    "print(f\"\\nüö´ Columns ONLY in Dataset B (removed in Dataset A):\")\n",
    "print(\"-\" * 70)\n",
    "if columns_in_B_not_A:\n",
    "    for col in sorted(columns_in_B_not_A):\n",
    "        print(f\"  - {col}\")\n",
    "else:\n",
    "    print(\"  None\")\n",
    "\n",
    "print(f\"\\n‚ûï Columns ONLY in Dataset A (not in Dataset B):\")\n",
    "print(\"-\" * 70)\n",
    "if columns_in_A_not_B:\n",
    "    for col in sorted(columns_in_A_not_B):\n",
    "        print(f\"  - {col}\")\n",
    "else:\n",
    "    print(\"  None (expected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "3Ô∏è‚É£ TARGET VARIABLE VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset B (Full) - Target distribution:\n",
      "----------------------------------------------------------------------\n",
      "  Class 0: 190,055 (82.71%)\n",
      "  Class 1:  4,629 ( 2.01%)\n",
      "  Class 2: 35,097 (15.27%)\n",
      "\n",
      "üìä Dataset A (Clean) - Target distribution:\n",
      "----------------------------------------------------------------------\n",
      "  Class 0: 190,055 (82.71%)\n",
      "  Class 1:  4,629 ( 2.01%)\n",
      "  Class 2: 35,097 (15.27%)\n",
      "\n",
      "‚úÖ Target values identical? True\n"
     ]
    }
   ],
   "source": [
    "# 3. Target variable verification\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3Ô∏è‚É£ TARGET VARIABLE VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä Dataset B (Full) - Target distribution:\")\n",
    "print(\"-\" * 70)\n",
    "for cls, count in df_full['Diabetes_012'].value_counts().sort_index().items():\n",
    "    pct = (count / len(df_full)) * 100\n",
    "    print(f\"  Class {int(cls)}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "print(\"\\nüìä Dataset A (Clean) - Target distribution:\")\n",
    "print(\"-\" * 70)\n",
    "for cls, count in df_clean['Diabetes_012'].value_counts().sort_index().items():\n",
    "    pct = (count / len(df_clean)) * 100\n",
    "    print(f\"  Class {int(cls)}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "# Check if targets are identical\n",
    "targets_identical = df_full['Diabetes_012'].equals(df_clean['Diabetes_012'])\n",
    "print(f\"\\n‚úÖ Target values identical? {targets_identical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "4Ô∏è‚É£ COMMON FEATURES - VALUE COMPARISON\n",
      "======================================================================\n",
      "\n",
      "üìã Checking 18 common features...\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Age: Identical\n",
      "‚úÖ AnyHealthcare: Identical\n",
      "‚úÖ BMI: Identical\n",
      "‚úÖ CholCheck: Identical\n",
      "‚úÖ Education: Identical\n",
      "  ... and 13 more features\n",
      "\n",
      "‚úÖ All common features identical? True\n"
     ]
    }
   ],
   "source": [
    "# 4. Check common features - do they have same values?\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"4Ô∏è‚É£ COMMON FEATURES - VALUE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get columns that exist in both datasets\n",
    "common_cols = list(set(df_full.columns) & set(df_clean.columns))\n",
    "common_cols.remove('Diabetes_012')  # Already checked target\n",
    "common_cols.sort()\n",
    "\n",
    "print(f\"\\nüìã Checking {len(common_cols)} common features...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "all_identical = True\n",
    "for col in common_cols[:5]:  # Show first 5 as examples\n",
    "    identical = df_full[col].equals(df_clean[col])\n",
    "    status = \"‚úÖ\" if identical else \"‚ùå\"\n",
    "    print(f\"{status} {col}: {'Identical' if identical else 'DIFFERENT'}\")\n",
    "    if not identical:\n",
    "        all_identical = False\n",
    "\n",
    "if len(common_cols) > 5:\n",
    "    print(f\"  ... and {len(common_cols) - 5} more features\")\n",
    "\n",
    "print(f\"\\n‚úÖ All common features identical? {all_identical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "5Ô∏è‚É£ DATA TYPES VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset B (Full):\n",
      "float64    22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìä Dataset A (Clean):\n",
      "float64    19\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÖ Both datasets have only numeric types? True\n"
     ]
    }
   ],
   "source": [
    "# 5. Data types verification\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"5Ô∏è‚É£ DATA TYPES VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä Dataset B (Full):\")\n",
    "print(df_full.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nüìä Dataset A (Clean):\")\n",
    "print(df_clean.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n‚úÖ Both datasets have only numeric types? \", end=\"\")\n",
    "numeric_only_B = all(df_full.dtypes.apply(lambda x: x.kind in 'iufc'))\n",
    "numeric_only_A = all(df_clean.dtypes.apply(lambda x: x.kind in 'iufc'))\n",
    "print(numeric_only_B and numeric_only_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "6Ô∏è‚É£ MISSING VALUES CHECK\n",
      "======================================================================\n",
      "\n",
      "Dataset B (Full): 0 missing values\n",
      "Dataset A (Clean): 0 missing values\n",
      "\n",
      "‚úÖ No missing values in either dataset? True\n"
     ]
    }
   ],
   "source": [
    "# 6. Missing values check\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"6Ô∏è‚É£ MISSING VALUES CHECK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "missing_B = df_full.isnull().sum().sum()\n",
    "missing_A = df_clean.isnull().sum().sum()\n",
    "\n",
    "print(f\"\\nDataset B (Full): {missing_B} missing values\")\n",
    "print(f\"Dataset A (Clean): {missing_A} missing values\")\n",
    "\n",
    "print(f\"\\n‚úÖ No missing values in either dataset? {missing_B == 0 and missing_A == 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "7Ô∏è‚É£ SAMPLE ROWS (First 3 rows)\n",
      "======================================================================\n",
      "\n",
      "üìã Dataset B (Full) - First 3 rows:\n",
      "----------------------------------------------------------------------\n",
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
      "1           0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
      "2           0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  \\\n",
      "0                   0.0           0.0     0.0      1.0                0.0   \n",
      "1                   0.0           1.0     0.0      0.0                0.0   \n",
      "2                   0.0           0.0     1.0      0.0                0.0   \n",
      "\n",
      "   AnyHealthcare  NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex  \\\n",
      "0            1.0          0.0      5.0      18.0      15.0       1.0  0.0   \n",
      "1            0.0          1.0      3.0       0.0       0.0       0.0  0.0   \n",
      "2            1.0          1.0      5.0      30.0      30.0       1.0  0.0   \n",
      "\n",
      "   Age  Education  Income  \n",
      "0  9.0        4.0     3.0  \n",
      "1  7.0        6.0     1.0  \n",
      "2  9.0        4.0     8.0  \n",
      "\n",
      "üìã Dataset A (Clean) - First 3 rows:\n",
      "----------------------------------------------------------------------\n",
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
      "1           0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
      "2           0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  \\\n",
      "0                   0.0           0.0     0.0      1.0                0.0   \n",
      "1                   0.0           1.0     0.0      0.0                0.0   \n",
      "2                   0.0           0.0     1.0      0.0                0.0   \n",
      "\n",
      "   AnyHealthcare  NoDocbcCost  MentHlth  Sex  Age  Education  Income  \n",
      "0            1.0          0.0      18.0  0.0  9.0        4.0     3.0  \n",
      "1            0.0          1.0       0.0  0.0  7.0        6.0     1.0  \n",
      "2            1.0          1.0      30.0  0.0  9.0        4.0     8.0  \n"
     ]
    }
   ],
   "source": [
    "# 7. Sample rows comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"7Ô∏è‚É£ SAMPLE ROWS (First 3 rows)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìã Dataset B (Full) - First 3 rows:\")\n",
    "print(\"-\" * 70)\n",
    "print(df_full.head(3))\n",
    "\n",
    "print(\"\\nüìã Dataset A (Clean) - First 3 rows:\")\n",
    "print(\"-\" * 70)\n",
    "print(df_clean.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ FINAL VERIFICATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìã Verification Checklist:\n",
      "----------------------------------------------------------------------\n",
      "‚úÖ Same number of rows\n",
      "‚úÖ Dataset B has 22 columns\n",
      "‚úÖ Dataset A has 19 columns\n",
      "‚úÖ Target values identical\n",
      "‚úÖ Common features identical\n",
      "‚úÖ No missing values\n",
      "‚úÖ All numeric types\n",
      "‚úÖ 3 features removed in A\n",
      "\n",
      "======================================================================\n",
      "üéâ ALL CHECKS PASSED - DATASETS ARE READY!\n",
      "\n",
      "Dataset B (Full):  229,781 rows √ó 22 cols (21 features + target)\n",
      "Dataset A (Clean): 229,781 rows √ó 19 cols (18 features + target)\n",
      "\n",
      "Removed from Dataset A: DiffWalk, GenHlth, PhysHlth\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 8. Final summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ FINAL VERIFICATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "checks = {\n",
    "    'Same number of rows': len(df_full) == len(df_clean),\n",
    "    'Dataset B has 22 columns': len(df_full.columns) == 22,\n",
    "    'Dataset A has 19 columns': len(df_clean.columns) == 19,\n",
    "    'Target values identical': df_full['Diabetes_012'].equals(df_clean['Diabetes_012']),\n",
    "    'Common features identical': all_identical,\n",
    "    'No missing values': missing_B == 0 and missing_A == 0,\n",
    "    'All numeric types': numeric_only_B and numeric_only_A,\n",
    "    '3 features removed in A': len(columns_in_B_not_A) == 3,\n",
    "}\n",
    "\n",
    "print(\"\\nüìã Verification Checklist:\")\n",
    "print(\"-\" * 70)\n",
    "for check, passed in checks.items():\n",
    "    status = \"‚úÖ\" if passed else \"‚ùå\"\n",
    "    print(f\"{status} {check}\")\n",
    "\n",
    "all_passed = all(checks.values())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if all_passed:\n",
    "    print(\"üéâ ALL CHECKS PASSED - DATASETS ARE READY!\")\n",
    "    print(\"\\nDataset B (Full):  229,781 rows √ó 22 cols (21 features + target)\")\n",
    "    print(\"Dataset A (Clean): 229,781 rows √ó 19 cols (18 features + target)\")\n",
    "    print(\"\\nRemoved from Dataset A: DiffWalk, GenHlth, PhysHlth\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SOME CHECKS FAILED - Review above\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Critical Analysis: Data Preparation Decisions\n",
    "\n",
    "### **What We Did:**\n",
    "1. Created two versions of the dataset\n",
    "2. Identified potentially leaky features based on clinical reasoning\n",
    "3. Prepared scaling strategy for distance-based algorithms\n",
    "4. Kept data in raw form (no derived features yet)\n",
    "\n",
    "### **Why We Made These Choices:**\n",
    "\n",
    "#### **1. Two Datasets Approach**\n",
    "**Rationale:**\n",
    "- **Dataset B (Full)** allows us to see maximum achievable performance\n",
    "- **Dataset A (Clean)** ensures our model works for real-world prevention\n",
    "- Comparing them reveals the impact of potentially leaky features\n",
    "\n",
    "**Theory (from lectures):**\n",
    "- \"Data Understanding\" phase in CRISP-DM requires understanding causal relationships\n",
    "- Target leakage occurs when features are consequences rather than causes\n",
    "- Models with leakage may fail in production even with high test accuracy\n",
    "\n",
    "#### **2. Features Identified as Potentially Leaky**\n",
    "\n",
    "**`DiffWalk` (Difficulty Walking):**\n",
    "- **Clinical reasoning:** Diabetic neuropathy causes nerve damage ‚Üí difficulty walking\n",
    "- **Risk:** High - this is a known complication of uncontrolled diabetes\n",
    "- **Decision:** Remove in Dataset A\n",
    "\n",
    "**`GenHlth` (General Health Rating):**\n",
    "- **Clinical reasoning:** Self-reported health naturally decreases after diabetes diagnosis\n",
    "- **Risk:** Medium - could be both cause and consequence\n",
    "- **Decision:** Remove in Dataset A to be conservative\n",
    "\n",
    "**`PhysHlth` (Days of Poor Physical Health):**\n",
    "- **Clinical reasoning:** Similar to GenHlth - likely affected by diabetes symptoms\n",
    "- **Risk:** Medium - measures consequences of disease\n",
    "- **Decision:** Remove in Dataset A\n",
    "\n",
    "#### **3. Why NOT Remove Other Features?**\n",
    "\n",
    "**`Stroke` and `HeartDiseaseorAttack` - Why we kept them:**\n",
    "- While diabetes increases cardiovascular risk, these can occur independently\n",
    "- They represent comorbidities rather than direct consequences\n",
    "- Removing them might hurt model performance without clear benefit\n",
    "- If results show issues, we can revisit this decision\n",
    "\n",
    "#### **4. No Derived Features (Yet)**\n",
    "**Rationale:**\n",
    "- Start simple - raw features first\n",
    "- Tree-based models (Random Forest, XGBoost) can capture non-linearities automatically\n",
    "- Feature engineering adds complexity - only worth it if baseline results are poor\n",
    "- Easier to debug and interpret with original features\n",
    "\n",
    "**Potential future features (if needed):**\n",
    "- BMI categories (WHO standard: Underweight, Normal, Overweight, Obese)\n",
    "- Age groups (Young, Middle-age, Senior)\n",
    "- Interaction terms (e.g., Age √ó BMI)\n",
    "\n",
    "### **Strengths of Our Approach:**\n",
    "- ‚úÖ **Transparent:** Clear documentation of which features removed and why\n",
    "- ‚úÖ **Scientific:** Based on clinical knowledge and causal reasoning\n",
    "- ‚úÖ **Flexible:** Can easily test both datasets and compare results\n",
    "- ‚úÖ **Practical:** Dataset A addresses real-world preventive screening use case\n",
    "- ‚úÖ **Simple:** No premature feature engineering\n",
    "\n",
    "### **Limitations:**\n",
    "- ‚ö†Ô∏è **Uncertainty:** We can't be 100% certain which features are truly leaky without domain expert validation\n",
    "- ‚ö†Ô∏è **Trade-off:** Dataset A might have lower accuracy, but is more ethically sound for prevention\n",
    "- ‚ö†Ô∏è **Binary decision:** We're either keeping or removing features - no \"partial\" use\n",
    "- ‚ö†Ô∏è **Other potential leakage:** Features like `Stroke` or `HeartDiseaseorAttack` might also have some leakage\n",
    "\n",
    "### **Implications for Model Development:**\n",
    "\n",
    "**Expected outcomes:**\n",
    "1. **Dataset B** will likely show higher accuracy (especially if leakage exists)\n",
    "2. **Dataset A** will show more realistic performance for preventive screening\n",
    "3. Large performance difference suggests significant leakage in removed features\n",
    "4. Small performance difference validates our conservative feature removal\n",
    "\n",
    "**Next steps:**\n",
    "1. Exploratory analysis to understand feature relationships\n",
    "2. Clustering to identify risk segments\n",
    "3. Classification on BOTH datasets\n",
    "4. Compare results and discuss implications\n",
    "\n",
    "### **Ethical Considerations:**\n",
    "- Using leaky features in production could lead to **false confidence** in predictions\n",
    "- Healthcare systems need models that work for **early detection**, not just diagnosis confirmation\n",
    "- Transparent documentation allows future researchers to make informed decisions\n",
    "- Our two-dataset approach balances academic rigor with practical applicability\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "**What we accomplished:**\n",
    "- ‚úÖ Created Dataset B (Full) - 21 features, all information\n",
    "- ‚úÖ Created Dataset A (Clean) - 18 features, removed potential leakage\n",
    "- ‚úÖ Prepared scaling strategy for modeling pipeline\n",
    "- ‚úÖ Saved both datasets for future analysis\n",
    "\n",
    "**Ready for:**\n",
    "- üìä Notebook 03: Exploratory Analysis\n",
    "- üîµ Notebook 04: Clustering\n",
    "- üéØ Notebook 05: Classification\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
