{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fefe51f",
   "metadata": {},
   "source": [
    "# üè• CDC Diabetes Health Indicators: 3-Class Classification & Clustering Analysis\n",
    "\n",
    "**Course:** WM9QG-15 - Fundamentals of Artificial Intelligence and Data Mining  \n",
    "**Institution:** University of Warwick  \n",
    "**Assessment:** Individual Project (70%)  \n",
    "\n",
    "---\n",
    "\n",
    "## üìã Project Overview\n",
    "\n",
    "**Context:** Contracted by public health research institute to analyze CDC BRFSS 2015 dataset (253,680 individuals)\n",
    "\n",
    "**Research Questions:**\n",
    "1. **Classification:** Can we predict diabetes diagnosis (No Diabetes, Prediabetes, Diabetes) based on health indicators?\n",
    "2. **Clustering:** Can we identify meaningful population segments at risk for targeted interventions?\n",
    "\n",
    "**Dataset:** CDC Behavioral Risk Factor Surveillance System (BRFSS) 2015\n",
    "- **Samples:** 253,680 survey responses\n",
    "- **Features:** 21 health and demographic indicators\n",
    "- **Target:** 3-class (0=No Diabetes, 1=Prediabetes, 2=Diabetes)\n",
    "- **Challenge:** Severe class imbalance (84.3%, 1.8%, 13.9%)\n",
    "\n",
    "**Approach:**\n",
    "- CRISP-DM inspired methodology (naturally structured)\n",
    "- Comprehensive comparison of 6 class imbalance strategies\n",
    "- Custom class weight optimization via Optuna\n",
    "- Feature selection using RFECV\n",
    "- Both unsupervised (clustering) and supervised (classification) learning\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Outcomes Addressed\n",
    "\n",
    "- **LO2:** Select and apply appropriate AI/ML algorithms\n",
    "- **LO3:** Critically evaluate different tools and techniques\n",
    "- **LO5:** Synthesize methodologies to articulate solution rationale\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Expected Deliverables\n",
    "\n",
    "1. Population clustering for risk segmentation\n",
    "2. Probabilistic classification model (predict_proba)\n",
    "3. Performance comparison before/after imbalance handling\n",
    "4. Optimal class weights via Bayesian optimization\n",
    "5. Critical evaluation of methodology\n",
    "6. Ethical and practical implications discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696450a",
   "metadata": {},
   "source": [
    "# üìë Table of Contents\n",
    "\n",
    "## 0Ô∏è‚É£ Setup & Configuration\n",
    "- 0.1 Import Libraries\n",
    "- 0.2 Load Configuration\n",
    "- 0.3 Helper Functions\n",
    "\n",
    "## 1Ô∏è‚É£ Data Understanding & Exploratory Analysis\n",
    "- 1.1 Load and Inspect Data\n",
    "- 1.2 Class Distribution Analysis\n",
    "- 1.3 Feature Distributions\n",
    "- 1.4 Correlation Analysis\n",
    "- 1.5 Target Leakage Investigation\n",
    "- 1.6 Critical Analysis: Data Quality\n",
    "\n",
    "## 2Ô∏è‚É£ Data Preparation\n",
    "- 2.1 Preprocessing Pipeline\n",
    "- 2.2 Feature Engineering\n",
    "- 2.3 Train/Validation/Test Split\n",
    "- 2.4 Critical Analysis: Preparation Decisions\n",
    "\n",
    "## 3Ô∏è‚É£ Clustering Analysis (Unsupervised Learning)\n",
    "- 3.1 K-Means Implementation\n",
    "- 3.2 DBSCAN Implementation\n",
    "- 3.3 Cluster Evaluation\n",
    "- 3.4 Cluster Interpretation for Public Health\n",
    "- 3.5 Critical Analysis: Clustering Results\n",
    "\n",
    "## 4Ô∏è‚É£ Classification - Baseline & Strategy Comparison\n",
    "- 4.1 Baseline Models (No Weighting)\n",
    "- 4.2 Strategy Comparison (6 approaches)\n",
    "- 4.3 Performance Evaluation\n",
    "- 4.4 Best Strategy Selection\n",
    "- 4.5 Critical Analysis: Imbalance Handling\n",
    "\n",
    "## 5Ô∏è‚É£ Feature Selection & Hyperparameter Optimization\n",
    "- 5.1 RFECV Feature Selection\n",
    "- 5.2 Optuna Weight + Hyperparameter Optimization\n",
    "- 5.3 Final Model Training\n",
    "- 5.4 Critical Analysis: Optimization Results\n",
    "\n",
    "## 6Ô∏è‚É£ Comprehensive Evaluation & Implications\n",
    "- 6.1 Model Performance Analysis\n",
    "- 6.2 Confusion Matrix Deep Dive\n",
    "- 6.3 Error Analysis\n",
    "- 6.4 Fairness & Bias Assessment\n",
    "- 6.5 Ethical Considerations\n",
    "- 6.6 Deployment Recommendations\n",
    "- 6.7 Critical Reflection: Limitations & Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d30658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab76a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "Python version: 3.12.5 (tags/v3.12.5:ff3bc82, Aug  6 2024, 20:45:27) [MSC v.1940 64 bit (AMD64)]\n",
      "Pandas version: 3.0.0\n",
      "NumPy version: 2.4.2\n",
      "Scikit-learn version: 1.8.0\n",
      "Optuna version: 4.7.0\n",
      "\n",
      "============================================================\n",
      "üéØ Configuration loaded from: config.py\n",
      "üìä Random seed: 42\n",
      "üìÅ Data path: data/CDC_Diabetes_Dataset.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS: STANDARD LIBRARIES\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS: SCIKIT-LEARN - PREPROCESSING\n",
    "# ============================================================================\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    StratifiedKFold,\n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS: SCIKIT-LEARN - MODELS\n",
    "# ============================================================================\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    VotingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Clustering models\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS: SCIKIT-LEARN - FEATURE SELECTION\n",
    "# ============================================================================\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, chi2\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS: SCIKIT-LEARN - METRICS\n",
    "# ============================================================================\n",
    "from sklearn.metrics import (\n",
    "    # Classification metrics\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    \n",
    "    # Clustering metrics\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    "    adjusted_rand_score,\n",
    "    normalized_mutual_info_score\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS: IMBALANCED-LEARN\n",
    "# ============================================================================\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS: OPTUNA (HYPERPARAMETER OPTIMIZATION)\n",
    "# ============================================================================\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances\n",
    ")\n",
    "\n",
    "# Suppress Optuna logging (optional)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS: SCIPY\n",
    "# ============================================================================\n",
    "from scipy import stats\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS: CONFIGURATION\n",
    "# ============================================================================\n",
    "import config\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY IMPORTS\n",
    "# ============================================================================\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üéØ Configuration loaded from: config.py\")\n",
    "print(f\"üìä Random seed: {config.RANDOM_STATE}\")\n",
    "print(f\"üìÅ Data path: {config.DATA_PATH}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bebec451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Display settings configured\n",
      "‚úÖ Output directories created\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DISPLAY SETTINGS\n",
    "# ============================================================================\n",
    "# Pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(config.COLOR_PALETTE)\n",
    "\n",
    "# Figure defaults\n",
    "plt.rcParams['figure.figsize'] = config.FIGURE_SIZE\n",
    "plt.rcParams['figure.dpi'] = config.FIGURE_DPI\n",
    "plt.rcParams['savefig.dpi'] = config.FIGURE_DPI\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE OUTPUT DIRECTORIES\n",
    "# ============================================================================\n",
    "for directory in [config.OUTPUT_DIR, config.MODELS_DIR, \n",
    "                  config.FIGURES_DIR, config.RESULTS_DIR]:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Display settings configured\")\n",
    "print(\"‚úÖ Output directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14966726",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Helper Functions\n",
    "\n",
    "The following helper functions will be used throughout the analysis to:\n",
    "1. **Visualization:** Plot distributions, confusion matrices, ROC curves\n",
    "2. **Evaluation:** Calculate and display comprehensive metrics\n",
    "3. **Comparison:** Compare multiple models systematically\n",
    "4. **Saving:** Save models, figures, and results consistently\n",
    "\n",
    "These functions follow the code style from course lab workshops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a94ab5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Visualization functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS: VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def plot_class_distribution(y, title=\"Class Distribution\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plot class distribution as bar chart and pie chart side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : array-like\n",
    "        Target variable\n",
    "    title : str\n",
    "        Plot title\n",
    "    save_path : str, optional\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Count and percentage\n",
    "    counts = pd.Series(y).value_counts().sort_index()\n",
    "    percentages = counts / len(y) * 100\n",
    "    \n",
    "    # Bar chart\n",
    "    axes[0].bar(counts.index, counts.values, \n",
    "                color=[config.CLASS_COLORS[i] for i in counts.index],\n",
    "                edgecolor='black', linewidth=1.5)\n",
    "    axes[0].set_xlabel('Class', fontsize=12)\n",
    "    axes[0].set_ylabel('Count', fontsize=12)\n",
    "    axes[0].set_title(f'{title} - Counts', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xticks(range(config.N_CLASSES))\n",
    "    axes[0].set_xticklabels(config.TARGET_NAMES)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, (idx, val) in enumerate(counts.items()):\n",
    "        axes[0].text(i, val + max(counts) * 0.01, f'{val:,}\\n({percentages[idx]:.1f}%)',\n",
    "                    ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(counts.values, labels=config.TARGET_NAMES,\n",
    "                colors=[config.CLASS_COLORS[i] for i in counts.index],\n",
    "                autopct='%1.1f%%', startangle=90,\n",
    "                textprops={'fontsize': 11, 'fontweight': 'bold'})\n",
    "    axes[1].set_title(f'{title} - Proportions', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"üíæ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Class Distribution Statistics\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for idx in counts.index:\n",
    "        print(f\"{config.TARGET_NAMES[idx]:20s}: {counts[idx]:8,} ({percentages[idx]:5.2f}%)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total samples: {len(y):,}\")\n",
    "    print(f\"Imbalance ratio (Class 1/Class 0): 1:{counts[0]/counts[1]:.1f}\")\n",
    "    print(f\"Imbalance ratio (Class 2/Class 0): 1:{counts[0]/counts[2]:.1f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "def plot_feature_distributions(df, features, target_col, ncols=3, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot distributions of multiple features, colored by target class.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset\n",
    "    features : list\n",
    "        List of feature names to plot\n",
    "    target_col : str\n",
    "        Target column name\n",
    "    ncols : int\n",
    "        Number of columns in subplot grid\n",
    "    save_path : str, optional\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    nrows = (len(features) + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(6*ncols, 4*nrows))\n",
    "    axes = axes.flatten() if len(features) > 1 else [axes]\n",
    "    \n",
    "    for idx, feature in enumerate(features):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Plot for each class\n",
    "        for class_val in sorted(df[target_col].unique()):\n",
    "            data = df[df[target_col] == class_val][feature]\n",
    "            ax.hist(data, bins=30, alpha=0.5, \n",
    "                   color=config.CLASS_COLORS[class_val],\n",
    "                   label=config.TARGET_NAMES[class_val],\n",
    "                   edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        ax.set_xlabel(feature, fontsize=11)\n",
    "        ax.set_ylabel('Frequency', fontsize=11)\n",
    "        ax.set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(features), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"üíæ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_matrix(df, features, title=\"Correlation Matrix\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plot correlation matrix heatmap.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset\n",
    "    features : list\n",
    "        List of features to include\n",
    "    title : str\n",
    "        Plot title\n",
    "    save_path : str, optional\n",
    "        Path to save figure\n",
    "    \"\"\"\n",
    "    # Calculate correlation\n",
    "    corr = df[features].corr()\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', \n",
    "                cmap='coolwarm', center=0, vmin=-1, vmax=1,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
    "                ax=ax)\n",
    "    \n",
    "    ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"üíæ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print highly correlated features\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Highly Correlated Feature Pairs (|r| > 0.5)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    high_corr = []\n",
    "    for i in range(len(corr.columns)):\n",
    "        for j in range(i+1, len(corr.columns)):\n",
    "            if abs(corr.iloc[i, j]) > 0.5:\n",
    "                high_corr.append((corr.columns[i], corr.columns[j], corr.iloc[i, j]))\n",
    "    \n",
    "    if high_corr:\n",
    "        for feat1, feat2, corr_val in sorted(high_corr, key=lambda x: abs(x[2]), reverse=True):\n",
    "            print(f\"{feat1:20s} <-> {feat2:20s}: {corr_val:6.3f}\")\n",
    "    else:\n",
    "        print(\"No feature pairs with |correlation| > 0.5\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(\"‚úÖ Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "039d9e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS: EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_classifier(y_true, y_pred, y_pred_proba=None, \n",
    "                       model_name=\"Model\", display_cm=True):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of classification model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    y_pred_proba : array-like, optional\n",
    "        Predicted probabilities (for ROC-AUC)\n",
    "    model_name : str\n",
    "        Name of the model for display\n",
    "    display_cm : bool\n",
    "        Whether to display confusion matrix\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary of metric scores\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä EVALUATION: {model_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'precision_weighted': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'recall_weighted': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "        'f1_weighted': f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # ROC-AUC (if probabilities provided)\n",
    "    if y_pred_proba is not None:\n",
    "        try:\n",
    "            metrics['roc_auc_ovr'] = roc_auc_score(y_true, y_pred_proba, \n",
    "                                                   multi_class='ovr', average='weighted')\n",
    "        except:\n",
    "            metrics['roc_auc_ovr'] = None\n",
    "    \n",
    "    # Display metrics\n",
    "    print(\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy:           {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy:  {metrics['balanced_accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro):   {metrics['f1_macro']:.4f} ‚≠ê\")\n",
    "    print(f\"  F1-Score (Weighted):{metrics['f1_weighted']:.4f}\")\n",
    "    if metrics.get('roc_auc_ovr'):\n",
    "        print(f\"  ROC-AUC (OVR):      {metrics['roc_auc_ovr']:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(\"Per-Class Performance:\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                                target_names=config.TARGET_NAMES,\n",
    "                                digits=4, zero_division=0))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    if display_cm:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                      display_labels=config.TARGET_NAMES)\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
    "        ax.set_title(f'Confusion Matrix - {model_name}', \n",
    "                    fontsize=14, fontweight='bold', pad=15)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compare_models(results_dict, metric='f1_macro', save_path=None):\n",
    "    \"\"\"\n",
    "    Compare multiple models side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict : dict\n",
    "        Dictionary of {model_name: metrics_dict}\n",
    "    metric : str\n",
    "        Primary metric for sorting\n",
    "    save_path : str, optional\n",
    "        Path to save comparison table\n",
    "    \"\"\"\n",
    "    # Create DataFrame\n",
    "    df_results = pd.DataFrame(results_dict).T\n",
    "    df_results = df_results.sort_values(metric, ascending=False)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üìä MODEL COMPARISON (Sorted by {metric})\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(df_results.to_string())\n",
    "    print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    # Highlight best model\n",
    "    best_model = df_results.index[0]\n",
    "    best_score = df_results.iloc[0][metric]\n",
    "    print(f\"üèÜ Best Model: {best_model}\")\n",
    "    print(f\"   {metric}: {best_score:.4f}\\n\")\n",
    "    \n",
    "    # Bar plot comparison\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(df_results))\n",
    "    width = 0.2\n",
    "    \n",
    "    metrics_to_plot = ['f1_macro', 'f1_weighted', 'balanced_accuracy']\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    for i, metric_name in enumerate(metrics_to_plot):\n",
    "        if metric_name in df_results.columns:\n",
    "            values = df_results[metric_name].values\n",
    "            ax.bar(x + i*width, values, width, label=metric_name, color=colors[i])\n",
    "    \n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(df_results.index, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=config.FIGURE_DPI, bbox_inches='tight')\n",
    "        print(f\"üíæ Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Save table\n",
    "    if save_path:\n",
    "        csv_path = save_path.replace('.png', '.csv')\n",
    "        df_results.to_csv(csv_path)\n",
    "        print(f\"üíæ Saved: {csv_path}\")\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "print(\"‚úÖ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f519cb45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ DATA UNDERSTANDING & EXPLORATORY ANALYSIS\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Load and inspect the CDC diabetes dataset\n",
    "2. Understand data structure, types, and quality\n",
    "3. Analyze class distribution and imbalance severity\n",
    "4. Explore feature distributions and relationships\n",
    "5. Identify correlations and potential multicollinearity\n",
    "6. Investigate potential target leakage features\n",
    "7. Document data quality issues and prepare for preprocessing\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Critical Analysis Preview\n",
    "\n",
    "Throughout this section, we will critically evaluate:\n",
    "- **Data quality:** Are there missing values? Duplicates? Outliers?\n",
    "- **Class imbalance:** How severe? What strategies might work?\n",
    "- **Feature relationships:** Which features correlate with target? With each other?\n",
    "- **Leakage risk:** Could any features be consequences rather than predictors of diabetes?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f71c263d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING DATASET\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/CDC_Diabetes_Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Dataset loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaros\\Desktop\\python\\faidm\\individual_project\\diabetes-classification-ml\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaros\\Desktop\\python\\faidm\\individual_project\\diabetes-classification-ml\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaros\\Desktop\\python\\faidm\\individual_project\\diabetes-classification-ml\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaros\\Desktop\\python\\faidm\\individual_project\\diabetes-classification-ml\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaros\\Desktop\\python\\faidm\\individual_project\\diabetes-classification-ml\\venv\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/CDC_Diabetes_Dataset.csv'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1.1 LOAD AND INSPECT DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(config.DATA_PATH)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"üìä Shape: {df.shape}\")\n",
    "print(f\"   Rows (samples): {df.shape[0]:,}\")\n",
    "print(f\"   Columns (features): {df.shape[1]}\")\n",
    "print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "# Display last few rows\n",
    "print(\"Last 5 rows:\")\n",
    "display(df.tail())\n",
    "\n",
    "print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "# Basic info\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(f\"\\n{'='*70}\\n\")\n",
    "\n",
    "# Column names\n",
    "print(\"Column names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1.2 DATA QUALITY ASSESSMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n1. Missing Values:\")\n",
    "print(\"-\" * 70)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Percentage': missing_pct\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df)\n",
    "    print(f\"\\n‚ö†Ô∏è  Found {len(missing_df)} columns with missing values\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"\\n2. Duplicate Rows:\")\n",
    "print(\"-\" * 70)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate rows: {duplicates:,} ({(duplicates/len(df)*100):.2f}%)\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"‚ö†Ô∏è  Warning: Found duplicate rows\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicate rows\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n3. Data Types:\")\n",
    "print(\"-\" * 70)\n",
    "print(df.dtypes)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n4. Summary Statistics:\")\n",
    "print(\"-\" * 70)\n",
    "display(df.describe())\n",
    "\n",
    "# Check for constant features (no variance)\n",
    "print(\"\\n5. Feature Variance Check:\")\n",
    "print(\"-\" * 70)\n",
    "constant_features = []\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() <= 1:\n",
    "        constant_features.append(col)\n",
    "\n",
    "if constant_features:\n",
    "    print(f\"‚ö†Ô∏è  Constant features (no variance): {constant_features}\")\n",
    "else:\n",
    "    print(\"‚úÖ All features have variance\")\n",
    "\n",
    "# Value range check\n",
    "print(\"\\n6. Feature Value Ranges:\")\n",
    "print(\"-\" * 70)\n",
    "for col in df.columns:\n",
    "    print(f\"{col:25s}: [{df[col].min():8.2f}, {df[col].max():8.2f}] \"\n",
    "          f\"(unique: {df[col].nunique()})\")\n",
    "\n",
    "print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c27af",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
